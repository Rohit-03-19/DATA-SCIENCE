{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "# Transformers with Keras\n",
    "\n",
    "Estimated time needed **45** mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to use the Keras library to build a transformer using a sequence-to-sequence architecture with self-attention for translation. We will train the model using a sample dataset and then use this model for English to Spanish translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives for this Notebook    \n",
    "* How to use the Keras library to build transformers model\n",
    "* Train the transformer model using a given dataset\n",
    "* Use the trained transformer model to translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 4>\n",
    "1. <a href=\"#Import-Keras-and-Packages\">Import Keras and Packages</a><br>\n",
    "2. <a href=\"#Step-1:-Data-Preparation\">Step 1: Data Preparation</a><br>\n",
    "3. <a href=\"#Step-2:-Self-Attention-Layer\">Step 2: Self-Attention Layer</a><br>\n",
    "4. <a href=\"#Step-3:-Model-Architecture\">Step 3: Model Architecture</a><br>\n",
    "5. <a href=\"#Step-4:-Training-the-Model\">Step 4: Training the Model</a><br>\n",
    "6. <a href=\"#Step-5:-Plotting-the-training-loss\">Step 5: Plotting the training loss</a><br>\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.1\n",
      "  Downloading tensorflow-2.17.1-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.17.1 (from tensorflow==2.17.1)\n",
      "  Downloading tensorflow_intel-2.17.1-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (0.45.1)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading optree-0.17.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.17.1-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.17.1-cp312-cp312-win_amd64.whl (382.4 MB)\n",
      "   ---------------------------------------- 0.0/382.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/382.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/382.4 MB 1.9 MB/s eta 0:03:26\n",
      "   ---------------------------------------- 1.0/382.4 MB 1.7 MB/s eta 0:03:41\n",
      "   ---------------------------------------- 1.3/382.4 MB 1.6 MB/s eta 0:03:53\n",
      "   ---------------------------------------- 1.8/382.4 MB 1.8 MB/s eta 0:03:36\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.4/382.4 MB 1.1 MB/s eta 0:06:00\n",
      "   ---------------------------------------- 2.9/382.4 MB 1.2 MB/s eta 0:05:17\n",
      "   ---------------------------------------- 3.1/382.4 MB 1.3 MB/s eta 0:05:03\n",
      "   ---------------------------------------- 3.7/382.4 MB 1.3 MB/s eta 0:04:56\n",
      "   ---------------------------------------- 4.2/382.4 MB 1.4 MB/s eta 0:04:37\n",
      "   ---------------------------------------- 4.5/382.4 MB 1.4 MB/s eta 0:04:36\n",
      "   ---------------------------------------- 4.7/382.4 MB 1.4 MB/s eta 0:04:36\n",
      "    --------------------------------------- 5.0/382.4 MB 1.4 MB/s eta 0:04:34\n",
      "    --------------------------------------- 5.2/382.4 MB 1.4 MB/s eta 0:04:36\n",
      "    --------------------------------------- 5.8/382.4 MB 1.4 MB/s eta 0:04:25\n",
      "    --------------------------------------- 6.0/382.4 MB 1.4 MB/s eta 0:04:22\n",
      "    --------------------------------------- 6.3/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "    --------------------------------------- 6.3/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "    --------------------------------------- 6.3/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "    --------------------------------------- 6.6/382.4 MB 1.3 MB/s eta 0:04:52\n",
      "    --------------------------------------- 6.8/382.4 MB 1.3 MB/s eta 0:04:52\n",
      "    --------------------------------------- 7.1/382.4 MB 1.3 MB/s eta 0:04:56\n",
      "    --------------------------------------- 7.3/382.4 MB 1.3 MB/s eta 0:04:56\n",
      "    --------------------------------------- 7.9/382.4 MB 1.3 MB/s eta 0:04:52\n",
      "    --------------------------------------- 8.1/382.4 MB 1.3 MB/s eta 0:04:48\n",
      "    --------------------------------------- 8.4/382.4 MB 1.3 MB/s eta 0:04:42\n",
      "    --------------------------------------- 8.4/382.4 MB 1.3 MB/s eta 0:04:42\n",
      "    --------------------------------------- 9.2/382.4 MB 1.3 MB/s eta 0:04:44\n",
      "    --------------------------------------- 9.4/382.4 MB 1.3 MB/s eta 0:04:38\n",
      "   - -------------------------------------- 10.0/382.4 MB 1.3 MB/s eta 0:04:36\n",
      "   - -------------------------------------- 10.5/382.4 MB 1.4 MB/s eta 0:04:30\n",
      "   - -------------------------------------- 11.0/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "   - -------------------------------------- 11.5/382.4 MB 1.5 MB/s eta 0:04:14\n",
      "   - -------------------------------------- 12.1/382.4 MB 1.5 MB/s eta 0:04:14\n",
      "   - -------------------------------------- 12.6/382.4 MB 1.5 MB/s eta 0:04:07\n",
      "   - -------------------------------------- 12.6/382.4 MB 1.5 MB/s eta 0:04:07\n",
      "   - -------------------------------------- 12.6/382.4 MB 1.5 MB/s eta 0:04:07\n",
      "   - -------------------------------------- 12.8/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "   - -------------------------------------- 13.4/382.4 MB 1.4 MB/s eta 0:04:17\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 14.2/382.4 MB 1.4 MB/s eta 0:04:31\n",
      "   - -------------------------------------- 14.9/382.4 MB 1.4 MB/s eta 0:04:22\n",
      "   - -------------------------------------- 15.7/382.4 MB 1.5 MB/s eta 0:04:11\n",
      "   - -------------------------------------- 16.5/382.4 MB 1.5 MB/s eta 0:04:05\n",
      "   - -------------------------------------- 16.8/382.4 MB 1.5 MB/s eta 0:04:02\n",
      "   - -------------------------------------- 17.6/382.4 MB 1.5 MB/s eta 0:03:58\n",
      "   - -------------------------------------- 18.1/382.4 MB 1.6 MB/s eta 0:03:55\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   -- ------------------------------------- 19.9/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 19.9/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 19.9/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 20.7/382.4 MB 1.5 MB/s eta 0:03:54\n",
      "   -- ------------------------------------- 21.0/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 22.0/382.4 MB 1.6 MB/s eta 0:03:46\n",
      "   -- ------------------------------------- 22.8/382.4 MB 1.6 MB/s eta 0:03:42\n",
      "   -- ------------------------------------- 23.9/382.4 MB 1.7 MB/s eta 0:03:35\n",
      "   -- ------------------------------------- 24.4/382.4 MB 1.7 MB/s eta 0:03:33\n",
      "   -- ------------------------------------- 25.2/382.4 MB 1.7 MB/s eta 0:03:27\n",
      "   -- ------------------------------------- 25.2/382.4 MB 1.7 MB/s eta 0:03:27\n",
      "   -- ------------------------------------- 25.7/382.4 MB 1.7 MB/s eta 0:03:30\n",
      "   -- ------------------------------------- 26.2/382.4 MB 1.7 MB/s eta 0:03:27\n",
      "   -- ------------------------------------- 27.3/382.4 MB 1.8 MB/s eta 0:03:22\n",
      "   -- ------------------------------------- 27.8/382.4 MB 1.8 MB/s eta 0:03:21\n",
      "   -- ------------------------------------- 28.3/382.4 MB 1.8 MB/s eta 0:03:18\n",
      "   --- ------------------------------------ 29.4/382.4 MB 1.8 MB/s eta 0:03:14\n",
      "   --- ------------------------------------ 30.4/382.4 MB 1.9 MB/s eta 0:03:10\n",
      "   --- ------------------------------------ 31.5/382.4 MB 1.9 MB/s eta 0:03:06\n",
      "   --- ------------------------------------ 31.5/382.4 MB 1.9 MB/s eta 0:03:06\n",
      "   --- ------------------------------------ 32.2/382.4 MB 1.9 MB/s eta 0:03:05\n",
      "   --- ------------------------------------ 33.6/382.4 MB 2.0 MB/s eta 0:02:59\n",
      "   --- ------------------------------------ 33.6/382.4 MB 2.0 MB/s eta 0:02:59\n",
      "   --- ------------------------------------ 34.6/382.4 MB 2.0 MB/s eta 0:02:57\n",
      "   --- ------------------------------------ 35.7/382.4 MB 2.0 MB/s eta 0:02:54\n",
      "   --- ------------------------------------ 35.7/382.4 MB 2.0 MB/s eta 0:02:54\n",
      "   --- ------------------------------------ 35.7/382.4 MB 2.0 MB/s eta 0:02:54\n",
      "   --- ------------------------------------ 35.9/382.4 MB 1.9 MB/s eta 0:02:59\n",
      "   --- ------------------------------------ 36.7/382.4 MB 2.0 MB/s eta 0:02:55\n",
      "   --- ------------------------------------ 37.7/382.4 MB 2.0 MB/s eta 0:02:53\n",
      "   --- ------------------------------------ 37.7/382.4 MB 2.0 MB/s eta 0:02:53\n",
      "   ---- ----------------------------------- 38.8/382.4 MB 2.0 MB/s eta 0:02:51\n",
      "   ---- ----------------------------------- 39.8/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 39.8/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 40.9/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 41.7/382.4 MB 2.1 MB/s eta 0:02:46\n",
      "   ---- ----------------------------------- 41.9/382.4 MB 2.1 MB/s eta 0:02:45\n",
      "   ---- ----------------------------------- 41.9/382.4 MB 2.1 MB/s eta 0:02:45\n",
      "   ---- ----------------------------------- 41.9/382.4 MB 2.1 MB/s eta 0:02:45\n",
      "   ---- ----------------------------------- 42.7/382.4 MB 2.0 MB/s eta 0:02:48\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 44.0/382.4 MB 2.0 MB/s eta 0:02:50\n",
      "   ---- ----------------------------------- 44.0/382.4 MB 2.0 MB/s eta 0:02:50\n",
      "   ---- ----------------------------------- 45.1/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 45.1/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.7/382.4 MB 2.0 MB/s eta 0:02:52\n",
      "   ---- ----------------------------------- 47.2/382.4 MB 2.0 MB/s eta 0:02:50\n",
      "   ----- ---------------------------------- 48.2/382.4 MB 2.0 MB/s eta 0:02:48\n",
      "   ----- ---------------------------------- 48.2/382.4 MB 2.0 MB/s eta 0:02:48\n",
      "   ----- ---------------------------------- 50.1/382.4 MB 2.0 MB/s eta 0:02:45\n",
      "   ----- ---------------------------------- 50.3/382.4 MB 2.0 MB/s eta 0:02:44\n",
      "   ----- ---------------------------------- 51.4/382.4 MB 2.1 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 52.4/382.4 MB 2.1 MB/s eta 0:02:40\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 54.3/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 54.5/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ----- ---------------------------------- 54.5/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ----- ---------------------------------- 54.5/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 56.6/382.4 MB 2.0 MB/s eta 0:02:43\n",
      "   ------ --------------------------------- 57.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 57.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 58.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 58.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 59.0/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ------ --------------------------------- 59.2/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ------ --------------------------------- 60.3/382.4 MB 2.0 MB/s eta 0:02:40\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.2/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 65.0/382.4 MB 2.1 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 66.1/382.4 MB 2.1 MB/s eta 0:02:31\n",
      "   ------ --------------------------------- 66.1/382.4 MB 2.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 67.1/382.4 MB 2.1 MB/s eta 0:02:30\n",
      "   ------- -------------------------------- 67.1/382.4 MB 2.1 MB/s eta 0:02:30\n",
      "   ------- -------------------------------- 67.1/382.4 MB 2.1 MB/s eta 0:02:30\n",
      "   ------- -------------------------------- 68.2/382.4 MB 2.1 MB/s eta 0:02:29\n",
      "   ------- -------------------------------- 69.2/382.4 MB 2.1 MB/s eta 0:02:27\n",
      "   ------- -------------------------------- 69.7/382.4 MB 2.1 MB/s eta 0:02:27\n",
      "   ------- -------------------------------- 70.3/382.4 MB 2.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 74.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 74.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 75.5/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 76.5/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 76.5/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 77.6/382.4 MB 2.2 MB/s eta 0:02:19\n",
      "   -------- ------------------------------- 77.6/382.4 MB 2.2 MB/s eta 0:02:19\n",
      "   -------- ------------------------------- 78.6/382.4 MB 2.2 MB/s eta 0:02:17\n",
      "   -------- ------------------------------- 79.7/382.4 MB 2.2 MB/s eta 0:02:15\n",
      "   -------- ------------------------------- 80.7/382.4 MB 2.3 MB/s eta 0:02:14\n",
      "   -------- ------------------------------- 80.7/382.4 MB 2.3 MB/s eta 0:02:14\n",
      "   -------- ------------------------------- 81.8/382.4 MB 2.3 MB/s eta 0:02:10\n",
      "   -------- ------------------------------- 82.8/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 82.8/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 83.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 83.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 84.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 84.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 88.1/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 88.1/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 89.1/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 91.2/382.4 MB 2.2 MB/s eta 0:02:12\n",
      "   --------- ------------------------------ 92.3/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 92.3/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 93.3/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   --------- ------------------------------ 93.8/382.4 MB 2.2 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 95.4/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 95.4/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 95.4/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   ---------- ----------------------------- 95.9/382.4 MB 2.2 MB/s eta 0:02:09\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 98.0/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 99.4/382.4 MB 2.2 MB/s eta 0:02:12\n",
      "   ---------- ----------------------------- 100.1/382.4 MB 2.2 MB/s eta 0:02:11\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.9/382.4 MB 2.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 101.7/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 101.7/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 101.7/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 104.9/382.4 MB 2.1 MB/s eta 0:02:13\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 107.0/382.4 MB 2.1 MB/s eta 0:02:13\n",
      "   ----------- ---------------------------- 108.0/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 108.0/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 109.1/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ----------- ---------------------------- 110.1/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ----------- ---------------------------- 110.1/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 114.3/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------ --------------------------- 114.8/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------ --------------------------- 115.3/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 116.4/382.4 MB 2.1 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 117.4/382.4 MB 2.1 MB/s eta 0:02:07\n",
      "   ------------ --------------------------- 117.4/382.4 MB 2.1 MB/s eta 0:02:07\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 120.6/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.9/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 123.7/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------ --------------------------- 123.7/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------- -------------------------- 124.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 124.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 124.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 125.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 126.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 126.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 128.2/382.4 MB 1.9 MB/s eta 0:02:14\n",
      "   ------------- -------------------------- 129.0/382.4 MB 1.9 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 129.0/382.4 MB 1.9 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 129.0/382.4 MB 1.9 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 130.0/382.4 MB 1.9 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 131.1/382.4 MB 2.0 MB/s eta 0:02:09\n",
      "   ------------- -------------------------- 132.1/382.4 MB 2.0 MB/s eta 0:02:06\n",
      "   ------------- -------------------------- 132.1/382.4 MB 2.0 MB/s eta 0:02:06\n",
      "   ------------- -------------------------- 133.2/382.4 MB 2.0 MB/s eta 0:02:05\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 135.5/382.4 MB 2.0 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 136.3/382.4 MB 2.0 MB/s eta 0:02:04\n",
      "   -------------- ------------------------- 137.1/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 137.1/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 137.1/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 139.5/382.4 MB 1.8 MB/s eta 0:02:13\n",
      "   -------------- ------------------------- 140.5/382.4 MB 1.9 MB/s eta 0:02:11\n",
      "   -------------- ------------------------- 140.5/382.4 MB 1.9 MB/s eta 0:02:11\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 142.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   --------------- ------------------------ 143.7/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 144.7/382.4 MB 1.9 MB/s eta 0:02:04\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 146.3/382.4 MB 1.9 MB/s eta 0:02:04\n",
      "   --------------- ------------------------ 146.8/382.4 MB 1.9 MB/s eta 0:02:03\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 148.9/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 148.9/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 149.7/382.4 MB 1.8 MB/s eta 0:02:08\n",
      "   --------------- ------------------------ 149.9/382.4 MB 1.8 MB/s eta 0:02:07\n",
      "   --------------- ------------------------ 151.0/382.4 MB 1.9 MB/s eta 0:02:05\n",
      "   --------------- ------------------------ 151.8/382.4 MB 1.8 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 152.0/382.4 MB 1.8 MB/s eta 0:02:05\n",
      "   ---------------- ----------------------- 153.1/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 153.1/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 154.1/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 154.1/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 154.1/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 156.2/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 157.3/382.4 MB 1.9 MB/s eta 0:01:58\n",
      "   ---------------- ----------------------- 157.3/382.4 MB 1.9 MB/s eta 0:01:58\n",
      "   ---------------- ----------------------- 158.3/382.4 MB 1.9 MB/s eta 0:01:56\n",
      "   ---------------- ----------------------- 159.4/382.4 MB 2.0 MB/s eta 0:01:55\n",
      "   ---------------- ----------------------- 160.4/382.4 MB 2.0 MB/s eta 0:01:52\n",
      "   ---------------- ----------------------- 160.4/382.4 MB 2.0 MB/s eta 0:01:52\n",
      "   ---------------- ----------------------- 161.5/382.4 MB 2.0 MB/s eta 0:01:51\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 163.6/382.4 MB 2.0 MB/s eta 0:01:52\n",
      "   ----------------- ---------------------- 164.6/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 164.6/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 164.6/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 167.8/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 168.8/382.4 MB 2.0 MB/s eta 0:01:49\n",
      "   ----------------- ---------------------- 168.8/382.4 MB 2.0 MB/s eta 0:01:49\n",
      "   ----------------- ---------------------- 169.9/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 169.9/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 169.9/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 170.1/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 171.4/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 172.0/382.4 MB 2.0 MB/s eta 0:01:45\n",
      "   ------------------ --------------------- 172.8/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 173.0/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 173.0/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 173.0/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 174.1/382.4 MB 2.0 MB/s eta 0:01:45\n",
      "   ------------------ --------------------- 175.1/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 175.1/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 176.2/382.4 MB 2.0 MB/s eta 0:01:44\n",
      "   ------------------ --------------------- 177.2/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 177.5/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 180.4/382.4 MB 2.1 MB/s eta 0:01:38\n",
      "   ------------------ --------------------- 181.4/382.4 MB 2.1 MB/s eta 0:01:36\n",
      "   ------------------ --------------------- 181.4/382.4 MB 2.1 MB/s eta 0:01:36\n",
      "   ------------------- -------------------- 182.2/382.4 MB 2.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 184.3/382.4 MB 2.1 MB/s eta 0:01:34\n",
      "   ------------------- -------------------- 184.5/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 184.5/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 185.6/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 185.6/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 185.6/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 186.4/382.4 MB 2.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 186.6/382.4 MB 2.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 187.7/382.4 MB 2.2 MB/s eta 0:01:31\n",
      "   ------------------- -------------------- 187.7/382.4 MB 2.2 MB/s eta 0:01:31\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 189.0/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 190.8/382.4 MB 2.1 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 190.8/382.4 MB 2.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 191.6/382.4 MB 2.2 MB/s eta 0:01:28\n",
      "   -------------------- ------------------- 191.9/382.4 MB 2.2 MB/s eta 0:01:28\n",
      "   -------------------- ------------------- 192.7/382.4 MB 2.2 MB/s eta 0:01:27\n",
      "   -------------------- ------------------- 194.0/382.4 MB 2.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 194.0/382.4 MB 2.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 195.0/382.4 MB 2.3 MB/s eta 0:01:24\n",
      "   -------------------- ------------------- 195.0/382.4 MB 2.3 MB/s eta 0:01:24\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 198.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 198.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 199.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 199.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 200.0/382.4 MB 2.1 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 200.3/382.4 MB 2.1 MB/s eta 0:01:26\n",
      "   --------------------- ------------------ 201.3/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   --------------------- ------------------ 202.1/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 203.4/382.4 MB 2.2 MB/s eta 0:01:22\n",
      "   --------------------- ------------------ 204.5/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 204.5/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 205.5/382.4 MB 2.2 MB/s eta 0:01:19\n",
      "   --------------------- ------------------ 206.3/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 206.6/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 207.6/382.4 MB 2.2 MB/s eta 0:01:19\n",
      "   --------------------- ------------------ 208.7/382.4 MB 2.3 MB/s eta 0:01:17\n",
      "   --------------------- ------------------ 208.7/382.4 MB 2.3 MB/s eta 0:01:17\n",
      "   --------------------- ------------------ 209.7/382.4 MB 2.3 MB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 210.8/382.4 MB 2.3 MB/s eta 0:01:15\n",
      "   ---------------------- ----------------- 211.8/382.4 MB 2.3 MB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 212.9/382.4 MB 2.3 MB/s eta 0:01:14\n",
      "   ---------------------- ----------------- 212.9/382.4 MB 2.3 MB/s eta 0:01:14\n",
      "   ---------------------- ----------------- 214.2/382.4 MB 2.3 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 215.0/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 215.7/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 217.1/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 217.1/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 218.1/382.4 MB 2.3 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 219.2/382.4 MB 2.4 MB/s eta 0:01:10\n",
      "   ----------------------- ---------------- 220.2/382.4 MB 2.4 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 220.2/382.4 MB 2.4 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 221.2/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 224.1/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 224.4/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 224.4/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 224.4/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 225.4/382.4 MB 2.3 MB/s eta 0:01:10\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 227.5/382.4 MB 2.2 MB/s eta 0:01:11\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ------------------------ --------------- 229.6/382.4 MB 2.2 MB/s eta 0:01:09\n",
      "   ------------------------ --------------- 230.7/382.4 MB 2.2 MB/s eta 0:01:09\n",
      "   ------------------------ --------------- 231.7/382.4 MB 2.3 MB/s eta 0:01:07\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 233.8/382.4 MB 2.3 MB/s eta 0:01:05\n",
      "   ------------------------ --------------- 233.8/382.4 MB 2.3 MB/s eta 0:01:05\n",
      "   ------------------------ --------------- 234.9/382.4 MB 2.3 MB/s eta 0:01:04\n",
      "   ------------------------ --------------- 234.9/382.4 MB 2.3 MB/s eta 0:01:04\n",
      "   ------------------------ --------------- 234.9/382.4 MB 2.3 MB/s eta 0:01:04\n",
      "   ------------------------ --------------- 235.9/382.4 MB 2.2 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 237.0/382.4 MB 2.3 MB/s eta 0:01:05\n",
      "   ------------------------ --------------- 237.8/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------ --------------- 238.0/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 239.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 239.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 240.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 240.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 241.2/382.4 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------------------- -------------- 241.2/382.4 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 243.3/382.4 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------------------- -------------- 244.3/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 246.4/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 246.4/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 247.5/382.4 MB 2.2 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 248.3/382.4 MB 2.2 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 248.5/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 248.5/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   -------------------------- ------------- 248.8/382.4 MB 2.2 MB/s eta 0:01:01\n",
      "   -------------------------- ------------- 250.3/382.4 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------------------- ------------- 250.6/382.4 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------------------- ------------- 251.7/382.4 MB 2.3 MB/s eta 0:00:58\n",
      "   -------------------------- ------------- 252.4/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 252.7/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 252.7/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 252.7/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 254.8/382.4 MB 2.2 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 255.6/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 256.9/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   -------------------------- ------------- 257.9/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 258.5/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 259.5/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   --------------------------- ------------ 260.0/382.4 MB 2.3 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 260.0/382.4 MB 2.3 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 261.1/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 261.9/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 262.1/382.4 MB 2.3 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 263.2/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 263.2/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 265.3/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 266.3/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 267.4/382.4 MB 2.3 MB/s eta 0:00:51\n",
      "   --------------------------- ------------ 267.4/382.4 MB 2.3 MB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 268.2/382.4 MB 2.3 MB/s eta 0:00:50\n",
      "   ---------------------------- ----------- 269.5/382.4 MB 2.3 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 269.5/382.4 MB 2.3 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 269.5/382.4 MB 2.3 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 270.5/382.4 MB 2.3 MB/s eta 0:00:48\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 273.4/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 273.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 273.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 274.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 274.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 274.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 275.8/382.4 MB 2.1 MB/s eta 0:00:50\n",
      "   ---------------------------- ----------- 276.8/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 277.9/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 278.9/382.4 MB 2.2 MB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 278.9/382.4 MB 2.2 MB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 280.2/382.4 MB 2.1 MB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 283.1/382.4 MB 2.2 MB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 284.2/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 285.0/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 285.2/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 286.3/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ------------------------------ --------- 287.3/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 287.3/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 288.4/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 288.4/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 288.4/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 289.4/382.4 MB 2.2 MB/s eta 0:00:42\n",
      "   ------------------------------ --------- 290.5/382.4 MB 2.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 291.5/382.4 MB 2.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 291.5/382.4 MB 2.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 292.6/382.4 MB 2.3 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 293.6/382.4 MB 2.3 MB/s eta 0:00:38\n",
      "   ------------------------------ --------- 294.1/382.4 MB 2.4 MB/s eta 0:00:38\n",
      "   ------------------------------ --------- 294.6/382.4 MB 2.4 MB/s eta 0:00:37\n",
      "   ------------------------------ --------- 295.4/382.4 MB 2.4 MB/s eta 0:00:37\n",
      "   ------------------------------- -------- 296.7/382.4 MB 2.4 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 297.8/382.4 MB 2.4 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 297.8/382.4 MB 2.4 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 298.8/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 300.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 302.0/382.4 MB 2.5 MB/s eta 0:00:33\n",
      "   ------------------------------- -------- 303.0/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 303.0/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 303.0/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.9/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 305.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   -------------------------------- ------- 306.2/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 308.3/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 308.3/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 310.4/382.4 MB 2.5 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 310.4/382.4 MB 2.5 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 312.5/382.4 MB 2.4 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 312.5/382.4 MB 2.4 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 312.5/382.4 MB 2.4 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 313.5/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 314.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 314.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 314.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 315.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 316.7/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 316.7/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 317.7/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 318.2/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 318.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 319.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 319.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 319.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 320.6/382.4 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 320.9/382.4 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 321.9/382.4 MB 2.5 MB/s eta 0:00:25\n",
      "   --------------------------------- ------ 321.9/382.4 MB 2.5 MB/s eta 0:00:25\n",
      "   --------------------------------- ------ 323.0/382.4 MB 2.4 MB/s eta 0:00:25\n",
      "   --------------------------------- ------ 324.0/382.4 MB 2.5 MB/s eta 0:00:24\n",
      "   --------------------------------- ------ 324.0/382.4 MB 2.5 MB/s eta 0:00:24\n",
      "   ---------------------------------- ----- 325.1/382.4 MB 2.4 MB/s eta 0:00:24\n",
      "   ---------------------------------- ----- 326.1/382.4 MB 2.5 MB/s eta 0:00:23\n",
      "   ---------------------------------- ----- 326.1/382.4 MB 2.5 MB/s eta 0:00:23\n",
      "   ---------------------------------- ----- 327.2/382.4 MB 2.5 MB/s eta 0:00:23\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 329.8/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 330.3/382.4 MB 2.5 MB/s eta 0:00:21\n",
      "   ---------------------------------- ----- 331.4/382.4 MB 2.5 MB/s eta 0:00:21\n",
      "   ---------------------------------- ----- 332.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ----------------------------------- ---- 335.5/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 336.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.4/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 338.7/382.4 MB 2.5 MB/s eta 0:00:18\n",
      "   ----------------------------------- ---- 339.7/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 339.7/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 339.7/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.6/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.8/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.8/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.8/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 342.4/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 342.9/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 343.9/382.4 MB 2.4 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 344.5/382.4 MB 2.5 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 345.8/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 347.1/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 347.6/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 348.1/382.4 MB 2.5 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 348.1/382.4 MB 2.5 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 348.9/382.4 MB 2.4 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 349.2/382.4 MB 2.4 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 350.2/382.4 MB 2.4 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 351.0/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 352.3/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 352.3/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 352.3/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 353.4/382.4 MB 2.5 MB/s eta 0:00:12\n",
      "   ------------------------------------ --- 353.4/382.4 MB 2.5 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 353.9/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 355.5/382.4 MB 2.3 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 356.0/382.4 MB 2.3 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 356.5/382.4 MB 2.3 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 357.3/382.4 MB 2.3 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 358.6/382.4 MB 2.3 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 358.6/382.4 MB 2.3 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 359.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 362.3/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 362.8/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 363.9/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 363.9/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 363.9/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 364.9/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 364.9/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 365.7/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 366.0/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 367.0/382.4 MB 2.3 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 367.0/382.4 MB 2.3 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 367.0/382.4 MB 2.3 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 367.8/382.4 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 368.1/382.4 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 369.1/382.4 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 370.1/382.4 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 371.2/382.4 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 371.2/382.4 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 372.2/382.4 MB 2.3 MB/s eta 0:00:05\n",
      "   ---------------------------------------  373.3/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  373.3/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  374.3/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  375.1/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  376.2/382.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------------------  376.4/382.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------------------  377.5/382.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  379.6/382.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  379.6/382.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  380.4/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 382.4/382.4 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 2.9/4.7 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.7/4.7 MB 12.4 MB/s eta 0:00:00\n",
      "Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 2.4/2.9 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 3.1/26.4 MB 15.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.0/26.4 MB 14.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.7/26.4 MB 13.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.5/26.4 MB 13.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.9/26.4 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.8/26.4 MB 13.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.2/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.6/15.5 MB 15.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.2/15.5 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.4/15.5 MB 14.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.1/15.5 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 4.2/5.5 MB 20.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-win_amd64.whl (314 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.31.1\n",
      "    Uninstalling protobuf-6.31.1:\n",
      "      Successfully uninstalled protobuf-6.31.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.11.3 libclang-18.1.1 markdown-3.9 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.1.0 numpy-1.26.4 opt-einsum-3.4.0 optree-0.17.0 protobuf-4.25.8 rich-14.2.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.1 tensorflow-intel-2.17.1 termcolor-3.2.0 werkzeug-3.1.3 wrapt-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\parid\\anaconda3\\envs\\Rohit\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\parid\\anaconda3\\envs\\Rohit\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.9.2 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "==== All required libraries are installed =====\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.1\n",
    "!pip install matplotlib==3.9.2\n",
    "\n",
    "print(\"==== All required libraries are installed =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress the tensorflow warning messages\n",
    "We use the following code to  suppress the warning messages due to use of CPU architechture for tensoflow.\n",
    "\n",
    "You may want to **comment out** these lines if you are using the GPU architechture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To use Keras, you will also need to install a backend framework – such as TensorFlow.\n",
    "\n",
    "If you install TensorFlow 2.16 or above, it will install Keras by default.\n",
    "\n",
    "We are using the CPU version of tensorflow since we are dealing with smaller datasets. \n",
    "You may install the GPU version of tensorflow on your machine to accelarate the processing of larger datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "We start by define the sentences and text for translation training\n",
    "Sentence Pairs: Defines a small dataset of English-Spanish sentence pairs.\n",
    "Target Sequences:\n",
    "Prepends \"startseq\" and appends \"endseq\" to each target sentence for the decoder to learn when to start and stop translating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parallel sentences (English -> Spanish)\n",
    "input_texts = [\n",
    "    \"Hello.\", \"How are you?\", \"I am learning machine translation.\", \"What is your name?\", \"I love programming.\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hola.\", \"¿Cómo estás?\", \"Estoy aprendiendo traducción automática.\", \"¿Cuál es tu nombre?\", \"Me encanta programar.\"\n",
    "]\n",
    "\n",
    "target_texts = [\"startseq \" + x + \" endseq\" for x in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we convert the text from the sentences to tokens and create a vocabulary\n",
    "Tokenization: Uses Tokenizer to convert words into numerical sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [3, 4, 5], [1, 6, 7, 8, 9], [10, 11, 12, 13], [1, 14, 15]]\n",
      "[[1, 3, 2], [1, 4, 5, 2], [1, 6, 7, 8, 9, 2], [1, 10, 11, 12, 13, 2], [1, 14, 15, 16, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "# print(input_tokenizer)\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "print(input_sequences)\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "print(output_sequences)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 14\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pad the corresponding sentences\n",
    "Padding: Ensures all sequences have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "max_input_length = max([len(seq) for seq in input_sequences])\n",
    "max_output_length = max([len(seq) for seq in output_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target data for training\n",
    "decoder_input_data = output_sequences[:, :-1]\n",
    "decoder_output_data = output_sequences[:, 1:]\n",
    "\n",
    "# Convert to one-hot\n",
    "decoder_output_data = np.array([np.eye(output_vocab_size)[seq] for seq in decoder_output_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Self-Attention Layer\n",
    "Self-attention is a mechanism that allows a model to **focus on relevant parts of the input sequence** while processing each word. This is particularly useful in:\n",
    "1) Machine Translation (e.g., aligning words correctly)\n",
    "2) Text Summarization\n",
    "3) Speech Recognition\n",
    "4) Image Processing (Vision Transformers)\n",
    "In this implementation, self-attention is used for text based sequence-to-sequence modeling.\n",
    "\n",
    "\n",
    "Self-Attention works for a given an input sequence by computing a weighted representation of all words for each position. It does so using three key components:\n",
    "\n",
    "1. Query **(Q)**, Key **(K)**, and Value **(V)** Matrices\n",
    "For each word (token) in a sequence:\n",
    "\n",
    "Query (Q): What this word is looking for.\n",
    "Key (K): What this word represents.\n",
    "Value (V): The actual information in the word.\n",
    "\n",
    "2. Compute **Attention Scores**\n",
    "Next, we **calculate the similarity between each query and key** using dot-product attention:\n",
    "Each word in a sequence attends to every other word based on these scores.\n",
    "\n",
    "3. Apply **Scaling & Softmax**\n",
    "Since dot-product values can be large, we scale them. \n",
    "Next, Applying softmax converts scores into attention weights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention class\n",
    "In this implementation of self-attention layer:\n",
    "1. We first initialize the weights in the **build** method, where:\n",
    "    1. **self.Wq**, **self.Wk**, **self.Wv** are the trainable weight matrices.\n",
    "    2. Their **shape is (feature_dim, feature_dim)**, meaning they transform input features into Q, K, and V representations.\n",
    "2. Applying Attention using **call** method. The **call()** method:\n",
    "   1. Computes **Q, K, V** by multiplying inputs (encoder/decoder output) with their respective weight matrices.\n",
    "   2. Computes **dot-product attention scores** using K.batch_dot(q, k, axes=[2, 2]), resulting in a (batch_size, seq_len, seq_len) matrix.\n",
    "   3. **Scales** the scores to avoid large values.\n",
    "   4. Applies **softmax** to normalize the attention scores.\n",
    "   5. **Multiplies attention weights with V** to get the final output.\n",
    "3. The **compute_output_shape** method defines the shape of the output tensor after the layer processes an input.\n",
    "    1. The output shape of the Self-Attention layer **remains the same** as the input shape.\n",
    "    2. The attention mechanism **transforms** the input but does not change its dimensions.4\n",
    "    3. If the attention layer changed the shape, you would modify compute_output_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Architecture\n",
    "The model follows an Encoder-Decoder structure:\n",
    "\n",
    "### Encoder:\n",
    "1) Takes input sentences (padded and tokenized).\n",
    "2) Uses an Embedding layer (word representations) + LSTM (to process sequences).\n",
    "    1. The LSTMs are used as the **help process variable-length input sentences** and generate meaningful translations.\n",
    "4) Outputs context vectors (hidden & cell states).\n",
    "\n",
    "### Attention Layer\n",
    "1) Applied to both the encoder and decoder outputs.\n",
    "2) Helps the decoder focus on relevant words during translation.\n",
    "\n",
    "### Decoder\n",
    "1) Receives target sequences (shifted one step ahead).\n",
    "2) Uses an LSTM with encoder states as initial states.\n",
    "3) Applies self-attention for better learning.\n",
    "4) Uses a Dense layer (Softmax) to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>) │                   │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,721</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m7,424\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,352\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mAdditiveAttention\u001b[0m) │                   │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m17\u001b[0m)     │      \u001b[38;5;34m8,721\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,377</span> (4.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,071,377\u001b[0m (4.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,377</span> (4.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,071,377\u001b[0m (4.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, Dense, Embedding, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    " \n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    " \n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    " \n",
    "# Attention: decoder attends to encoder outputs\n",
    "attention = AdditiveAttention()\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    " \n",
    "# Combine decoder outputs with attention context\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    " \n",
    "# Final Dense layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    " \n",
    "# Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training the Model\n",
    "Uses categorical_crossentropy as the loss function since output words are one-hot encoded.\n",
    "Trains using Adam optimizer for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0400 - loss: 2.8364\n",
      "Epoch 2/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3600 - loss: 2.8063\n",
      "Epoch 3/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.3600 - loss: 2.7751\n",
      "Epoch 4/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.3200 - loss: 2.7402\n",
      "Epoch 5/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.3200 - loss: 2.6985\n",
      "Epoch 6/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.3200 - loss: 2.6463\n",
      "Epoch 7/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.3200 - loss: 2.5788\n",
      "Epoch 8/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.3200 - loss: 2.4899\n",
      "Epoch 9/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2800 - loss: 2.3746\n",
      "Epoch 10/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.2800 - loss: 2.2364\n",
      "Epoch 11/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.2800 - loss: 2.1064\n",
      "Epoch 12/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2800 - loss: 2.0426\n",
      "Epoch 13/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2800 - loss: 2.0361\n",
      "Epoch 14/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2800 - loss: 1.9975\n",
      "Epoch 15/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3200 - loss: 1.9243\n",
      "Epoch 16/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4000 - loss: 1.8558\n",
      "Epoch 17/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5200 - loss: 1.7960\n",
      "Epoch 18/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5200 - loss: 1.7331\n",
      "Epoch 19/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5600 - loss: 1.6699\n",
      "Epoch 20/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5600 - loss: 1.6104\n",
      "Epoch 21/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.5600 - loss: 1.5544\n",
      "Epoch 22/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.4400 - loss: 1.4981\n",
      "Epoch 23/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.4400 - loss: 1.4389\n",
      "Epoch 24/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.4400 - loss: 1.3765\n",
      "Epoch 25/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.4800 - loss: 1.3118\n",
      "Epoch 26/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.4800 - loss: 1.2439\n",
      "Epoch 27/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5200 - loss: 1.1713\n",
      "Epoch 28/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5600 - loss: 1.0965\n",
      "Epoch 29/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6400 - loss: 1.0270\n",
      "Epoch 30/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7200 - loss: 0.9672\n",
      "Epoch 31/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7600 - loss: 0.9128\n",
      "Epoch 32/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7600 - loss: 0.8552\n",
      "Epoch 33/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8400 - loss: 0.7912\n",
      "Epoch 34/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8800 - loss: 0.7248\n",
      "Epoch 35/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9200 - loss: 0.6650\n",
      "Epoch 36/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9600 - loss: 0.6149\n",
      "Epoch 37/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9200 - loss: 0.5690\n",
      "Epoch 38/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9600 - loss: 0.5260\n",
      "Epoch 39/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.4869\n",
      "Epoch 40/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.4469\n",
      "Epoch 41/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.4064\n",
      "Epoch 42/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.3716\n",
      "Epoch 43/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.3445\n",
      "Epoch 44/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.3192\n",
      "Epoch 45/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.2919\n",
      "Epoch 46/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.2670\n",
      "Epoch 47/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.2432\n",
      "Epoch 48/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.2219\n",
      "Epoch 49/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.2037\n",
      "Epoch 50/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.1860\n",
      "Epoch 51/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.1694\n",
      "Epoch 52/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.1544\n",
      "Epoch 53/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.1410\n",
      "Epoch 54/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.1289\n",
      "Epoch 55/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.1176\n",
      "Epoch 56/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.1063\n",
      "Epoch 57/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0965\n",
      "Epoch 58/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0882\n",
      "Epoch 59/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0804\n",
      "Epoch 60/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0739\n",
      "Epoch 61/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0681\n",
      "Epoch 62/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0626\n",
      "Epoch 63/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0575\n",
      "Epoch 64/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0528\n",
      "Epoch 65/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0486\n",
      "Epoch 66/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0449\n",
      "Epoch 67/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0415\n",
      "Epoch 68/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0385\n",
      "Epoch 69/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0358\n",
      "Epoch 70/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0332\n",
      "Epoch 71/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0308\n",
      "Epoch 72/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0288\n",
      "Epoch 73/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0269\n",
      "Epoch 74/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.0253\n",
      "Epoch 75/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0237\n",
      "Epoch 76/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0223\n",
      "Epoch 77/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0210\n",
      "Epoch 78/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0198\n",
      "Epoch 79/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0187\n",
      "Epoch 80/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0177\n",
      "Epoch 81/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0168\n",
      "Epoch 82/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0159\n",
      "Epoch 83/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0152\n",
      "Epoch 84/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0145\n",
      "Epoch 85/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0138\n",
      "Epoch 86/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0132\n",
      "Epoch 87/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0126\n",
      "Epoch 88/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0121\n",
      "Epoch 89/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0116\n",
      "Epoch 90/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0112\n",
      "Epoch 91/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0107\n",
      "Epoch 92/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0103\n",
      "Epoch 93/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0100\n",
      "Epoch 94/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0096\n",
      "Epoch 95/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0093\n",
      "Epoch 96/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0090\n",
      "Epoch 97/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0087\n",
      "Epoch 98/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0085\n",
      "Epoch 99/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.0082\n",
      "Epoch 100/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0080\n",
      "Epoch 101/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0077\n",
      "Epoch 102/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0075\n",
      "Epoch 103/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0073\n",
      "Epoch 104/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0071\n",
      "Epoch 105/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0069\n",
      "Epoch 106/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0068\n",
      "Epoch 107/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0066\n",
      "Epoch 108/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.0064\n",
      "Epoch 109/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0063\n",
      "Epoch 110/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0061\n",
      "Epoch 111/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0060\n",
      "Epoch 112/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.0059\n",
      "Epoch 113/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0057\n",
      "Epoch 114/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0056\n",
      "Epoch 115/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0055\n",
      "Epoch 116/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0054\n",
      "Epoch 117/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0053\n",
      "Epoch 118/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0052\n",
      "Epoch 119/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0051\n",
      "Epoch 120/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0050\n",
      "Epoch 121/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "Epoch 122/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0048\n",
      "Epoch 123/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0047\n",
      "Epoch 124/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0046\n",
      "Epoch 125/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 126/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 127/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 128/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0043\n",
      "Epoch 129/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0042\n",
      "Epoch 130/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 131/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 132/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0040\n",
      "Epoch 133/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0039\n",
      "Epoch 134/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0039\n",
      "Epoch 135/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 136/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 137/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0037\n",
      "Epoch 138/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.0036\n",
      "Epoch 139/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0036\n",
      "Epoch 140/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 141/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 142/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Epoch 143/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Epoch 144/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 145/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 146/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 147/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 148/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 149/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 150/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 151/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 152/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 153/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0029\n",
      "Epoch 154/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0029\n",
      "Epoch 155/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0029\n",
      "Epoch 156/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 157/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 158/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 159/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 160/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 161/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 162/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 163/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 164/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 165/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 166/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 167/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 168/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 169/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 170/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 171/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 172/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 173/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 174/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 175/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 176/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 177/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 178/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 179/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 180/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 181/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 182/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 183/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 184/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 185/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 186/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 187/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 188/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 189/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 190/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 191/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 192/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 193/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 194/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 195/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 196/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 197/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 198/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 199/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 200/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 201/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 202/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 203/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 204/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 205/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 206/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 207/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 208/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 209/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 210/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 211/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 212/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 213/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 214/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 215/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 216/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 217/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 218/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 219/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 220/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 221/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 222/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 223/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 224/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 225/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 226/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 227/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 228/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 229/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 230/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 231/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 232/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 233/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 234/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 235/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 236/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 237/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 238/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 239/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 240/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 241/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 242/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 243/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 244/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 245/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 246/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 247/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 248/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 249/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 250/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 251/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 252/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 253/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 254/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 255/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 256/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 257/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 258/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 259/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 260/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 261/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 262/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 263/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 264/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 265/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 266/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 267/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 268/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 269/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 270/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 271/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 9.9665e-04\n",
      "Epoch 272/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 9.8994e-04\n",
      "Epoch 273/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 9.8328e-04\n",
      "Epoch 274/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 9.7672e-04\n",
      "Epoch 275/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 9.7019e-04\n",
      "Epoch 276/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 9.6378e-04\n",
      "Epoch 277/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 9.5740e-04\n",
      "Epoch 278/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 9.5110e-04\n",
      "Epoch 279/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 9.4484e-04\n",
      "Epoch 280/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 9.3868e-04\n",
      "Epoch 281/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 9.3256e-04\n",
      "Epoch 282/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 9.2653e-04\n",
      "Epoch 283/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 9.2051e-04\n",
      "Epoch 284/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 9.1459e-04\n",
      "Epoch 285/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 9.0876e-04\n",
      "Epoch 286/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 9.0289e-04\n",
      "Epoch 287/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 8.9718e-04\n",
      "Epoch 288/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 8.9147e-04\n",
      "Epoch 289/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 8.8584e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 8.8027e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 8.7477e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 8.6928e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 8.6388e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 8.5852e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 8.5321e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 8.4796e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 8.4273e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 8.3758e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 8.3251e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 8.2743e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "history_glorot_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=300, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plotting the training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome, now you have succesfully trained a transformers model.\n",
    "### Now let's try some practice excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, let's train the model using \"he_uniform\" initializer instead of \"glorot_uniform\". Then, compare the training loss between model using \"glorot_uniform\" vs \"he_uniform\" initializers by plotting them using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0400 - loss: 2.8364\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2400 - loss: 2.8047\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.2400 - loss: 2.7715\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.2400 - loss: 2.7336\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.2400 - loss: 2.6874\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2400 - loss: 2.6284\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2400 - loss: 2.5508\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2400 - loss: 2.4485\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.2400 - loss: 2.3196\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.2400 - loss: 2.1848\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2400 - loss: 2.1062\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2400 - loss: 2.1076\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2400 - loss: 2.0834\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2400 - loss: 2.0103\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2800 - loss: 1.9307\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3600 - loss: 1.8760\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.4400 - loss: 1.8402\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4400 - loss: 1.8019\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.4400 - loss: 1.7516\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5200 - loss: 1.6899\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.4800 - loss: 1.6217\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4800 - loss: 1.5535\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4400 - loss: 1.4902\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.4400 - loss: 1.4323\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4400 - loss: 1.3732\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.4400 - loss: 1.3053\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5600 - loss: 1.2278\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6000 - loss: 1.1476\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6800 - loss: 1.0727\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6800 - loss: 1.0053\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6800 - loss: 0.9416\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6800 - loss: 0.8819\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7200 - loss: 0.8288\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8000 - loss: 0.7731\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8400 - loss: 0.7149\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9200 - loss: 0.6632\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9200 - loss: 0.6144\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.5629\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.5181\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.4788\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.4398\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.4041\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.3718\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.3411\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.3141\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.2886\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.2639\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.2438\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.2256\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.2069\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.1909\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.1762\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.1616\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.1485\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.1365\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1255\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.1160\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.1069\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0983\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0905\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0828\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0760\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0703\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0649\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0599\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0552\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0509\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0470\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0437\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0406\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0378\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0354\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0330\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0308\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0289\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0271\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0255\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0240\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0227\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0214\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0202\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0192\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0182\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0173\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0165\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0157\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.0150\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0143\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0137\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0131\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0126\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0121\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0117\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0112\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0108\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0104\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0101\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0097\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0094\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0091\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write your answer here\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "\n",
    "#Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, try to use adaptive gradient optimizer instead of adam. Then, plot and compare the results between adam and adaptive gradient optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n",
    "plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2024-11-20  | 1.0  | Aman  |  Created the lab |\n",
    "<hr>\n",
    "-->\n",
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rohit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "3a07fb4049049613c9f3bf3a0aaeeac466433593dd808e2778bab531403fe8a9"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
