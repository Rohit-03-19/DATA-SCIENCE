{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "# Transformers with Keras\n",
    "\n",
    "Estimated time needed **45** mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to use the Keras library to build a transformer using a sequence-to-sequence architecture with self-attention for translation. We will train the model using a sample dataset and then use this model for English to Spanish translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives for this Notebook    \n",
    "* How to use the Keras library to build transformers model\n",
    "* Train the transformer model using a given dataset\n",
    "* Use the trained transformer model to translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 4>\n",
    "1. <a href=\"#Import-Keras-and-Packages\">Import Keras and Packages</a><br>\n",
    "2. <a href=\"#Step-1:-Data-Preparation\">Step 1: Data Preparation</a><br>\n",
    "3. <a href=\"#Step-2:-Self-Attention-Layer\">Step 2: Self-Attention Layer</a><br>\n",
    "4. <a href=\"#Step-3:-Model-Architecture\">Step 3: Model Architecture</a><br>\n",
    "5. <a href=\"#Step-4:-Training-the-Model\">Step 4: Training the Model</a><br>\n",
    "6. <a href=\"#Step-5:-Plotting-the-training-loss\">Step 5: Plotting the training loss</a><br>\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.1\n",
      "  Downloading tensorflow-2.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.17.1)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.76.0)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow==2.17.1)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.11.3)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow==2.17.1)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.17.1) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.1) (0.1.2)\n",
      "Downloading tensorflow-2.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.4/601.4 MB\u001b[0m \u001b[31m950.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: protobuf, numpy, tensorboard, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cpu 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 protobuf-4.25.8 tensorboard-2.17.1 tensorflow-2.17.1\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "==== All required libraries are installed =====\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.1\n",
    "!pip install matplotlib==3.9.2\n",
    "\n",
    "print(\"==== All required libraries are installed =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress the tensorflow warning messages\n",
    "We use the following code to  suppress the warning messages due to use of CPU architechture for tensoflow.\n",
    "\n",
    "You may want to **comment out** these lines if you are using the GPU architechture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To use Keras, you will also need to install a backend framework – such as TensorFlow.\n",
    "\n",
    "If you install TensorFlow 2.16 or above, it will install Keras by default.\n",
    "\n",
    "We are using the CPU version of tensorflow since we are dealing with smaller datasets. \n",
    "You may install the GPU version of tensorflow on your machine to accelarate the processing of larger datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 15:01:22.572259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-24 15:01:22.592033: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-24 15:01:22.597838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "We start by define the sentences and text for translation training\n",
    "Sentence Pairs: Defines a small dataset of English-Spanish sentence pairs.\n",
    "Target Sequences:\n",
    "Prepends \"startseq\" and appends \"endseq\" to each target sentence for the decoder to learn when to start and stop translating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parallel sentences (English -> Spanish)\n",
    "input_texts = [\n",
    "    \"Hello.\", \"How are you?\", \"I am learning machine translation.\", \"What is your name?\", \"I love programming.\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hola.\", \"¿Cómo estás?\", \"Estoy aprendiendo traducción automática.\", \"¿Cuál es tu nombre?\", \"Me encanta programar.\"\n",
    "]\n",
    "\n",
    "target_texts = [\"startseq \" + x + \" endseq\" for x in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we convert the text from the sentences to tokens and create a vocabulary\n",
    "Tokenization: Uses Tokenizer to convert words into numerical sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [3, 4, 5], [1, 6, 7, 8, 9], [10, 11, 12, 13], [1, 14, 15]]\n",
      "[[1, 3, 2], [1, 4, 5, 2], [1, 6, 7, 8, 9, 2], [1, 10, 11, 12, 13, 2], [1, 14, 15, 16, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "# print(input_tokenizer)\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "print(input_sequences)\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "print(output_sequences)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 14\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pad the corresponding sentences\n",
    "Padding: Ensures all sequences have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "max_input_length = max([len(seq) for seq in input_sequences])\n",
    "max_output_length = max([len(seq) for seq in output_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target data for training\n",
    "decoder_input_data = output_sequences[:, :-1]\n",
    "decoder_output_data = output_sequences[:, 1:]\n",
    "\n",
    "# Convert to one-hot\n",
    "decoder_output_data = np.array([np.eye(output_vocab_size)[seq] for seq in decoder_output_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Self-Attention Layer\n",
    "Self-attention is a mechanism that allows a model to **focus on relevant parts of the input sequence** while processing each word. This is particularly useful in:\n",
    "1) Machine Translation (e.g., aligning words correctly)\n",
    "2) Text Summarization\n",
    "3) Speech Recognition\n",
    "4) Image Processing (Vision Transformers)\n",
    "In this implementation, self-attention is used for text based sequence-to-sequence modeling.\n",
    "\n",
    "\n",
    "Self-Attention works for a given an input sequence by computing a weighted representation of all words for each position. It does so using three key components:\n",
    "\n",
    "1. Query **(Q)**, Key **(K)**, and Value **(V)** Matrices\n",
    "For each word (token) in a sequence:\n",
    "\n",
    "Query (Q): What this word is looking for.\n",
    "Key (K): What this word represents.\n",
    "Value (V): The actual information in the word.\n",
    "\n",
    "2. Compute **Attention Scores**\n",
    "Next, we **calculate the similarity between each query and key** using dot-product attention:\n",
    "Each word in a sequence attends to every other word based on these scores.\n",
    "\n",
    "3. Apply **Scaling & Softmax**\n",
    "Since dot-product values can be large, we scale them. \n",
    "Next, Applying softmax converts scores into attention weights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention class\n",
    "In this implementation of self-attention layer:\n",
    "1. We first initialize the weights in the **build** method, where:\n",
    "    1. **self.Wq**, **self.Wk**, **self.Wv** are the trainable weight matrices.\n",
    "    2. Their **shape is (feature_dim, feature_dim)**, meaning they transform input features into Q, K, and V representations.\n",
    "2. Applying Attention using **call** method. The **call()** method:\n",
    "   1. Computes **Q, K, V** by multiplying inputs (encoder/decoder output) with their respective weight matrices.\n",
    "   2. Computes **dot-product attention scores** using K.batch_dot(q, k, axes=[2, 2]), resulting in a (batch_size, seq_len, seq_len) matrix.\n",
    "   3. **Scales** the scores to avoid large values.\n",
    "   4. Applies **softmax** to normalize the attention scores.\n",
    "   5. **Multiplies attention weights with V** to get the final output.\n",
    "3. The **compute_output_shape** method defines the shape of the output tensor after the layer processes an input.\n",
    "    1. The output shape of the Self-Attention layer **remains the same** as the input shape.\n",
    "    2. The attention mechanism **transforms** the input but does not change its dimensions.4\n",
    "    3. If the attention layer changed the shape, you would modify compute_output_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Architecture\n",
    "The model follows an Encoder-Decoder structure:\n",
    "\n",
    "### Encoder:\n",
    "1) Takes input sentences (padded and tokenized).\n",
    "2) Uses an Embedding layer (word representations) + LSTM (to process sequences).\n",
    "    1. The LSTMs are used as the **help process variable-length input sentences** and generate meaningful translations.\n",
    "4) Outputs context vectors (hidden & cell states).\n",
    "\n",
    "### Attention Layer\n",
    "1) Applied to both the encoder and decoder outputs.\n",
    "2) Helps the decoder focus on relevant words during translation.\n",
    "\n",
    "### Decoder\n",
    "1) Receives target sequences (shifted one step ahead).\n",
    "2) Uses an LSTM with encoder states as initial states.\n",
    "3) Applies self-attention for better learning.\n",
    "4) Uses a Dense layer (Softmax) to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_6         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>) │                   │            │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,721</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m7,424\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_6         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,352\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ additive_attention… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mAdditiveAttention\u001b[0m) │                   │            │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ additive_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m17\u001b[0m)     │      \u001b[38;5;34m8,721\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,377</span> (4.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,071,377\u001b[0m (4.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,377</span> (4.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,071,377\u001b[0m (4.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, Dense, Embedding, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    " \n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    " \n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    " \n",
    "# Attention: decoder attends to encoder outputs\n",
    "attention = AdditiveAttention()\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    " \n",
    "# Combine decoder outputs with attention context\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    " \n",
    "# Final Dense layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    " \n",
    "# Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training the Model\n",
    "Uses categorical_crossentropy as the loss function since output words are one-hot encoded.\n",
    "Trains using Adam optimizer for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0800 - loss: 2.8322\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3600 - loss: 2.8017\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3600 - loss: 2.7695\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.3600 - loss: 2.7327\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.3600 - loss: 2.6880\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3200 - loss: 2.6312\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3200 - loss: 2.5567\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3200 - loss: 2.4581\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3200 - loss: 2.3308\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.2400 - loss: 2.1847\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.2400 - loss: 2.0651\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.2400 - loss: 2.0320\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.2400 - loss: 2.0406\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.2400 - loss: 2.0071\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3200 - loss: 1.9432\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.4400 - loss: 1.8769\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4400 - loss: 1.8112\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4400 - loss: 1.7489\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.5200 - loss: 1.6936\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5600 - loss: 1.6440\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6000 - loss: 1.5957\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.5200 - loss: 1.5446\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.5200 - loss: 1.4888\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.4800 - loss: 1.4292\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.4800 - loss: 1.3671\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.4800 - loss: 1.3014\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.4800 - loss: 1.2288\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.5600 - loss: 1.1501\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6000 - loss: 1.0732\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.6800 - loss: 1.0066\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7200 - loss: 0.9490\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7200 - loss: 0.8906\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7200 - loss: 0.8303\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8000 - loss: 0.7760\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8000 - loss: 0.7241\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8400 - loss: 0.6718\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9200 - loss: 0.6259\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9200 - loss: 0.5822\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9600 - loss: 0.5345\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9600 - loss: 0.4916\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.4533\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.4167\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.3856\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.3538\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.3235\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.2984\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.2756\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.2527\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.2315\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.2120\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.1948\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.1794\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.1641\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.1502\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.1371\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.1268\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.1167\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.1069\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0982\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0902\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.0835\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0773\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0714\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0661\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0612\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0570\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0531\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0494\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0460\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0428\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.0399\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.0374\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0351\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0329\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0309\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0290\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0274\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0258\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0244\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.0231\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0219\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0208\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0198\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0188\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0179\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0171\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0164\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0157\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0150\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0144\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0138\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0133\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.0128\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.0123\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0119\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 1.0000 - loss: 0.0114\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0111\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0107\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0103\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "history_glorot_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plotting the training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome, now you have succesfully trained a transformers model.\n",
    "### Now let's try some practice excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, let's train the model using \"he_uniform\" initializer instead of \"glorot_uniform\". Then, compare the training loss between model using \"glorot_uniform\" vs \"he_uniform\" initializers by plotting them using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0400 - loss: 2.8334\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3600 - loss: 2.8022\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.3200 - loss: 2.7691\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3200 - loss: 2.7310\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.3200 - loss: 2.6843\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.2800 - loss: 2.6247\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.2800 - loss: 2.5463\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.2800 - loss: 2.4431\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.2800 - loss: 2.3139\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.2800 - loss: 2.1798\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.2400 - loss: 2.1002\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.2400 - loss: 2.0935\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.2800 - loss: 2.0604\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.2800 - loss: 1.9819\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.3200 - loss: 1.9017\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.4400 - loss: 1.8544\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.4400 - loss: 1.8263\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.4400 - loss: 1.7856\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.4400 - loss: 1.7281\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.5200 - loss: 1.6636\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.5200 - loss: 1.6010\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.4400 - loss: 1.5448\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.4000 - loss: 1.4942\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.4000 - loss: 1.4444\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.4000 - loss: 1.3889\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.4400 - loss: 1.3235\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.4400 - loss: 1.2491\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.5600 - loss: 1.1700\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.6800 - loss: 1.0921\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6800 - loss: 1.0210\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.7200 - loss: 0.9588\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8000 - loss: 0.9013\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8000 - loss: 0.8438\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8400 - loss: 0.7880\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8400 - loss: 0.7353\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8400 - loss: 0.6827\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8800 - loss: 0.6309\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.5823\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.5380\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.4989\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.4608\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.4231\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.3897\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.3578\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.3264\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.2988\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.2754\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.2529\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.2315\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.2142\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.1976\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.1801\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.1641\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.1502\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.1369\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.1256\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.1155\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.1051\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0956\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0878\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0802\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0733\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0675\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0621\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0570\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.0526\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0487\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0449\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0415\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0385\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0359\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0333\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0311\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0291\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0272\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0255\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0239\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0224\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0212\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0200\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0189\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0179\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0170\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0161\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.0154\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.0146\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0139\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0133\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0128\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.0122\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0117\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0113\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0108\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0104\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0101\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0097\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0094\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0091\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0088\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0085\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write your answer here\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "\n",
    "#Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, try to use adaptive gradient optimizer instead of adam. Then, plot and compare the results between adam and adaptive gradient optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n",
    "plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2024-11-20  | 1.0  | Aman  |  Created the lab |\n",
    "<hr>\n",
    "-->\n",
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "3a07fb4049049613c9f3bf3a0aaeeac466433593dd808e2778bab531403fe8a9"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
