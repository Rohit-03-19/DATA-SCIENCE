{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "# Transformers with Keras\n",
    "\n",
    "Estimated time needed **45** mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to use the Keras library to build a transformer using a sequence-to-sequence architecture with self-attention for translation. We will train the model using a sample dataset and then use this model for English to Spanish translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives for this Notebook    \n",
    "* How to use the Keras library to build transformers model\n",
    "* Train the transformer model using a given dataset\n",
    "* Use the trained transformer model to translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 4>\n",
    "1. <a href=\"#Import-Keras-and-Packages\">Import Keras and Packages</a><br>\n",
    "2. <a href=\"#Step-1:-Data-Preparation\">Step 1: Data Preparation</a><br>\n",
    "3. <a href=\"#Step-2:-Self-Attention-Layer\">Step 2: Self-Attention Layer</a><br>\n",
    "4. <a href=\"#Step-3:-Model-Architecture\">Step 3: Model Architecture</a><br>\n",
    "5. <a href=\"#Step-4:-Training-the-Model\">Step 4: Training the Model</a><br>\n",
    "6. <a href=\"#Step-5:-Plotting-the-training-loss\">Step 5: Plotting the training loss</a><br>\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.1\n",
      "  Downloading tensorflow-2.17.1-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.17.1 (from tensorflow==2.17.1)\n",
      "  Downloading tensorflow_intel-2.17.1-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (0.45.1)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading optree-0.17.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.17.1-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.17.1-cp312-cp312-win_amd64.whl (382.4 MB)\n",
      "   ---------------------------------------- 0.0/382.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/382.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/382.4 MB 1.9 MB/s eta 0:03:26\n",
      "   ---------------------------------------- 1.0/382.4 MB 1.7 MB/s eta 0:03:41\n",
      "   ---------------------------------------- 1.3/382.4 MB 1.6 MB/s eta 0:03:53\n",
      "   ---------------------------------------- 1.8/382.4 MB 1.8 MB/s eta 0:03:36\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.1/382.4 MB 1.7 MB/s eta 0:03:44\n",
      "   ---------------------------------------- 2.4/382.4 MB 1.1 MB/s eta 0:06:00\n",
      "   ---------------------------------------- 2.9/382.4 MB 1.2 MB/s eta 0:05:17\n",
      "   ---------------------------------------- 3.1/382.4 MB 1.3 MB/s eta 0:05:03\n",
      "   ---------------------------------------- 3.7/382.4 MB 1.3 MB/s eta 0:04:56\n",
      "   ---------------------------------------- 4.2/382.4 MB 1.4 MB/s eta 0:04:37\n",
      "   ---------------------------------------- 4.5/382.4 MB 1.4 MB/s eta 0:04:36\n",
      "   ---------------------------------------- 4.7/382.4 MB 1.4 MB/s eta 0:04:36\n",
      "    --------------------------------------- 5.0/382.4 MB 1.4 MB/s eta 0:04:34\n",
      "    --------------------------------------- 5.2/382.4 MB 1.4 MB/s eta 0:04:36\n",
      "    --------------------------------------- 5.8/382.4 MB 1.4 MB/s eta 0:04:25\n",
      "    --------------------------------------- 6.0/382.4 MB 1.4 MB/s eta 0:04:22\n",
      "    --------------------------------------- 6.3/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "    --------------------------------------- 6.3/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "    --------------------------------------- 6.3/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "    --------------------------------------- 6.6/382.4 MB 1.3 MB/s eta 0:04:52\n",
      "    --------------------------------------- 6.8/382.4 MB 1.3 MB/s eta 0:04:52\n",
      "    --------------------------------------- 7.1/382.4 MB 1.3 MB/s eta 0:04:56\n",
      "    --------------------------------------- 7.3/382.4 MB 1.3 MB/s eta 0:04:56\n",
      "    --------------------------------------- 7.9/382.4 MB 1.3 MB/s eta 0:04:52\n",
      "    --------------------------------------- 8.1/382.4 MB 1.3 MB/s eta 0:04:48\n",
      "    --------------------------------------- 8.4/382.4 MB 1.3 MB/s eta 0:04:42\n",
      "    --------------------------------------- 8.4/382.4 MB 1.3 MB/s eta 0:04:42\n",
      "    --------------------------------------- 9.2/382.4 MB 1.3 MB/s eta 0:04:44\n",
      "    --------------------------------------- 9.4/382.4 MB 1.3 MB/s eta 0:04:38\n",
      "   - -------------------------------------- 10.0/382.4 MB 1.3 MB/s eta 0:04:36\n",
      "   - -------------------------------------- 10.5/382.4 MB 1.4 MB/s eta 0:04:30\n",
      "   - -------------------------------------- 11.0/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "   - -------------------------------------- 11.5/382.4 MB 1.5 MB/s eta 0:04:14\n",
      "   - -------------------------------------- 12.1/382.4 MB 1.5 MB/s eta 0:04:14\n",
      "   - -------------------------------------- 12.6/382.4 MB 1.5 MB/s eta 0:04:07\n",
      "   - -------------------------------------- 12.6/382.4 MB 1.5 MB/s eta 0:04:07\n",
      "   - -------------------------------------- 12.6/382.4 MB 1.5 MB/s eta 0:04:07\n",
      "   - -------------------------------------- 12.8/382.4 MB 1.4 MB/s eta 0:04:23\n",
      "   - -------------------------------------- 13.4/382.4 MB 1.4 MB/s eta 0:04:17\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 13.6/382.4 MB 1.5 MB/s eta 0:04:13\n",
      "   - -------------------------------------- 14.2/382.4 MB 1.4 MB/s eta 0:04:31\n",
      "   - -------------------------------------- 14.9/382.4 MB 1.4 MB/s eta 0:04:22\n",
      "   - -------------------------------------- 15.7/382.4 MB 1.5 MB/s eta 0:04:11\n",
      "   - -------------------------------------- 16.5/382.4 MB 1.5 MB/s eta 0:04:05\n",
      "   - -------------------------------------- 16.8/382.4 MB 1.5 MB/s eta 0:04:02\n",
      "   - -------------------------------------- 17.6/382.4 MB 1.5 MB/s eta 0:03:58\n",
      "   - -------------------------------------- 18.1/382.4 MB 1.6 MB/s eta 0:03:55\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   - -------------------------------------- 18.9/382.4 MB 1.6 MB/s eta 0:03:48\n",
      "   -- ------------------------------------- 19.9/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 19.9/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 19.9/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 20.7/382.4 MB 1.5 MB/s eta 0:03:54\n",
      "   -- ------------------------------------- 21.0/382.4 MB 1.6 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 22.0/382.4 MB 1.6 MB/s eta 0:03:46\n",
      "   -- ------------------------------------- 22.8/382.4 MB 1.6 MB/s eta 0:03:42\n",
      "   -- ------------------------------------- 23.9/382.4 MB 1.7 MB/s eta 0:03:35\n",
      "   -- ------------------------------------- 24.4/382.4 MB 1.7 MB/s eta 0:03:33\n",
      "   -- ------------------------------------- 25.2/382.4 MB 1.7 MB/s eta 0:03:27\n",
      "   -- ------------------------------------- 25.2/382.4 MB 1.7 MB/s eta 0:03:27\n",
      "   -- ------------------------------------- 25.7/382.4 MB 1.7 MB/s eta 0:03:30\n",
      "   -- ------------------------------------- 26.2/382.4 MB 1.7 MB/s eta 0:03:27\n",
      "   -- ------------------------------------- 27.3/382.4 MB 1.8 MB/s eta 0:03:22\n",
      "   -- ------------------------------------- 27.8/382.4 MB 1.8 MB/s eta 0:03:21\n",
      "   -- ------------------------------------- 28.3/382.4 MB 1.8 MB/s eta 0:03:18\n",
      "   --- ------------------------------------ 29.4/382.4 MB 1.8 MB/s eta 0:03:14\n",
      "   --- ------------------------------------ 30.4/382.4 MB 1.9 MB/s eta 0:03:10\n",
      "   --- ------------------------------------ 31.5/382.4 MB 1.9 MB/s eta 0:03:06\n",
      "   --- ------------------------------------ 31.5/382.4 MB 1.9 MB/s eta 0:03:06\n",
      "   --- ------------------------------------ 32.2/382.4 MB 1.9 MB/s eta 0:03:05\n",
      "   --- ------------------------------------ 33.6/382.4 MB 2.0 MB/s eta 0:02:59\n",
      "   --- ------------------------------------ 33.6/382.4 MB 2.0 MB/s eta 0:02:59\n",
      "   --- ------------------------------------ 34.6/382.4 MB 2.0 MB/s eta 0:02:57\n",
      "   --- ------------------------------------ 35.7/382.4 MB 2.0 MB/s eta 0:02:54\n",
      "   --- ------------------------------------ 35.7/382.4 MB 2.0 MB/s eta 0:02:54\n",
      "   --- ------------------------------------ 35.7/382.4 MB 2.0 MB/s eta 0:02:54\n",
      "   --- ------------------------------------ 35.9/382.4 MB 1.9 MB/s eta 0:02:59\n",
      "   --- ------------------------------------ 36.7/382.4 MB 2.0 MB/s eta 0:02:55\n",
      "   --- ------------------------------------ 37.7/382.4 MB 2.0 MB/s eta 0:02:53\n",
      "   --- ------------------------------------ 37.7/382.4 MB 2.0 MB/s eta 0:02:53\n",
      "   ---- ----------------------------------- 38.8/382.4 MB 2.0 MB/s eta 0:02:51\n",
      "   ---- ----------------------------------- 39.8/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 39.8/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 40.9/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 41.7/382.4 MB 2.1 MB/s eta 0:02:46\n",
      "   ---- ----------------------------------- 41.9/382.4 MB 2.1 MB/s eta 0:02:45\n",
      "   ---- ----------------------------------- 41.9/382.4 MB 2.1 MB/s eta 0:02:45\n",
      "   ---- ----------------------------------- 41.9/382.4 MB 2.1 MB/s eta 0:02:45\n",
      "   ---- ----------------------------------- 42.7/382.4 MB 2.0 MB/s eta 0:02:48\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 43.0/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 44.0/382.4 MB 2.0 MB/s eta 0:02:50\n",
      "   ---- ----------------------------------- 44.0/382.4 MB 2.0 MB/s eta 0:02:50\n",
      "   ---- ----------------------------------- 45.1/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 45.1/382.4 MB 2.0 MB/s eta 0:02:49\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.1/382.4 MB 2.0 MB/s eta 0:02:47\n",
      "   ---- ----------------------------------- 46.7/382.4 MB 2.0 MB/s eta 0:02:52\n",
      "   ---- ----------------------------------- 47.2/382.4 MB 2.0 MB/s eta 0:02:50\n",
      "   ----- ---------------------------------- 48.2/382.4 MB 2.0 MB/s eta 0:02:48\n",
      "   ----- ---------------------------------- 48.2/382.4 MB 2.0 MB/s eta 0:02:48\n",
      "   ----- ---------------------------------- 50.1/382.4 MB 2.0 MB/s eta 0:02:45\n",
      "   ----- ---------------------------------- 50.3/382.4 MB 2.0 MB/s eta 0:02:44\n",
      "   ----- ---------------------------------- 51.4/382.4 MB 2.1 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 52.4/382.4 MB 2.1 MB/s eta 0:02:40\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 53.5/382.4 MB 2.1 MB/s eta 0:02:38\n",
      "   ----- ---------------------------------- 54.3/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 54.5/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ----- ---------------------------------- 54.5/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ----- ---------------------------------- 54.5/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 55.6/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ----- ---------------------------------- 56.6/382.4 MB 2.0 MB/s eta 0:02:43\n",
      "   ------ --------------------------------- 57.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 57.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 58.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 58.7/382.4 MB 2.0 MB/s eta 0:02:41\n",
      "   ------ --------------------------------- 59.0/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ------ --------------------------------- 59.2/382.4 MB 2.0 MB/s eta 0:02:42\n",
      "   ------ --------------------------------- 60.3/382.4 MB 2.0 MB/s eta 0:02:40\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 61.9/382.4 MB 2.1 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.0/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 64.2/382.4 MB 2.1 MB/s eta 0:02:35\n",
      "   ------ --------------------------------- 65.0/382.4 MB 2.1 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 66.1/382.4 MB 2.1 MB/s eta 0:02:31\n",
      "   ------ --------------------------------- 66.1/382.4 MB 2.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 67.1/382.4 MB 2.1 MB/s eta 0:02:30\n",
      "   ------- -------------------------------- 67.1/382.4 MB 2.1 MB/s eta 0:02:30\n",
      "   ------- -------------------------------- 67.1/382.4 MB 2.1 MB/s eta 0:02:30\n",
      "   ------- -------------------------------- 68.2/382.4 MB 2.1 MB/s eta 0:02:29\n",
      "   ------- -------------------------------- 69.2/382.4 MB 2.1 MB/s eta 0:02:27\n",
      "   ------- -------------------------------- 69.7/382.4 MB 2.1 MB/s eta 0:02:27\n",
      "   ------- -------------------------------- 70.3/382.4 MB 2.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 71.3/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 72.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 73.4/382.4 MB 2.2 MB/s eta 0:02:21\n",
      "   ------- -------------------------------- 74.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 74.4/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 75.5/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 76.5/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 76.5/382.4 MB 2.2 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 77.6/382.4 MB 2.2 MB/s eta 0:02:19\n",
      "   -------- ------------------------------- 77.6/382.4 MB 2.2 MB/s eta 0:02:19\n",
      "   -------- ------------------------------- 78.6/382.4 MB 2.2 MB/s eta 0:02:17\n",
      "   -------- ------------------------------- 79.7/382.4 MB 2.2 MB/s eta 0:02:15\n",
      "   -------- ------------------------------- 80.7/382.4 MB 2.3 MB/s eta 0:02:14\n",
      "   -------- ------------------------------- 80.7/382.4 MB 2.3 MB/s eta 0:02:14\n",
      "   -------- ------------------------------- 81.8/382.4 MB 2.3 MB/s eta 0:02:10\n",
      "   -------- ------------------------------- 82.8/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 82.8/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 83.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 83.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 84.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 84.9/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   -------- ------------------------------- 86.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 87.0/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 88.1/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 88.1/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 89.1/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 90.2/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 91.2/382.4 MB 2.2 MB/s eta 0:02:12\n",
      "   --------- ------------------------------ 92.3/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 92.3/382.4 MB 2.3 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 93.3/382.4 MB 2.3 MB/s eta 0:02:08\n",
      "   --------- ------------------------------ 93.8/382.4 MB 2.2 MB/s eta 0:02:09\n",
      "   --------- ------------------------------ 95.4/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 95.4/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   --------- ------------------------------ 95.4/382.4 MB 2.3 MB/s eta 0:02:07\n",
      "   ---------- ----------------------------- 95.9/382.4 MB 2.2 MB/s eta 0:02:09\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 97.5/382.4 MB 2.2 MB/s eta 0:02:08\n",
      "   ---------- ----------------------------- 98.0/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 99.4/382.4 MB 2.2 MB/s eta 0:02:12\n",
      "   ---------- ----------------------------- 100.1/382.4 MB 2.2 MB/s eta 0:02:11\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.7/382.4 MB 2.2 MB/s eta 0:02:10\n",
      "   ---------- ----------------------------- 100.9/382.4 MB 2.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 101.7/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 101.7/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 101.7/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 102.8/382.4 MB 2.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 104.9/382.4 MB 2.1 MB/s eta 0:02:13\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 105.9/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 107.0/382.4 MB 2.1 MB/s eta 0:02:13\n",
      "   ----------- ---------------------------- 108.0/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 108.0/382.4 MB 2.1 MB/s eta 0:02:11\n",
      "   ----------- ---------------------------- 109.1/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ----------- ---------------------------- 110.1/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ----------- ---------------------------- 110.1/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 111.1/382.4 MB 2.1 MB/s eta 0:02:09\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 112.2/382.4 MB 2.0 MB/s eta 0:02:16\n",
      "   ----------- ---------------------------- 114.3/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------ --------------------------- 114.8/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------ --------------------------- 115.3/382.4 MB 2.1 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 116.4/382.4 MB 2.1 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 117.4/382.4 MB 2.1 MB/s eta 0:02:07\n",
      "   ------------ --------------------------- 117.4/382.4 MB 2.1 MB/s eta 0:02:07\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 118.5/382.4 MB 2.1 MB/s eta 0:02:04\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 119.5/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 120.6/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.6/382.4 MB 2.0 MB/s eta 0:02:08\n",
      "   ------------ --------------------------- 121.9/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 122.7/382.4 MB 2.0 MB/s eta 0:02:10\n",
      "   ------------ --------------------------- 123.7/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------ --------------------------- 123.7/382.4 MB 2.0 MB/s eta 0:02:13\n",
      "   ------------- -------------------------- 124.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 124.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 124.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 125.8/382.4 MB 2.0 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 126.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 126.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 127.9/382.4 MB 2.0 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 128.2/382.4 MB 1.9 MB/s eta 0:02:14\n",
      "   ------------- -------------------------- 129.0/382.4 MB 1.9 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 129.0/382.4 MB 1.9 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 129.0/382.4 MB 1.9 MB/s eta 0:02:12\n",
      "   ------------- -------------------------- 130.0/382.4 MB 1.9 MB/s eta 0:02:11\n",
      "   ------------- -------------------------- 131.1/382.4 MB 2.0 MB/s eta 0:02:09\n",
      "   ------------- -------------------------- 132.1/382.4 MB 2.0 MB/s eta 0:02:06\n",
      "   ------------- -------------------------- 132.1/382.4 MB 2.0 MB/s eta 0:02:06\n",
      "   ------------- -------------------------- 133.2/382.4 MB 2.0 MB/s eta 0:02:05\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 134.2/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 135.5/382.4 MB 2.0 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 136.3/382.4 MB 2.0 MB/s eta 0:02:04\n",
      "   -------------- ------------------------- 137.1/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 137.1/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 137.1/382.4 MB 2.0 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 137.4/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 138.4/382.4 MB 1.9 MB/s eta 0:02:10\n",
      "   -------------- ------------------------- 139.5/382.4 MB 1.8 MB/s eta 0:02:13\n",
      "   -------------- ------------------------- 140.5/382.4 MB 1.9 MB/s eta 0:02:11\n",
      "   -------------- ------------------------- 140.5/382.4 MB 1.9 MB/s eta 0:02:11\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 141.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   -------------- ------------------------- 142.6/382.4 MB 1.9 MB/s eta 0:02:08\n",
      "   --------------- ------------------------ 143.7/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 144.7/382.4 MB 1.9 MB/s eta 0:02:04\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 145.8/382.4 MB 2.0 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 146.3/382.4 MB 1.9 MB/s eta 0:02:04\n",
      "   --------------- ------------------------ 146.8/382.4 MB 1.9 MB/s eta 0:02:03\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 147.8/382.4 MB 1.9 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 148.9/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 148.9/382.4 MB 1.9 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 149.7/382.4 MB 1.8 MB/s eta 0:02:08\n",
      "   --------------- ------------------------ 149.9/382.4 MB 1.8 MB/s eta 0:02:07\n",
      "   --------------- ------------------------ 151.0/382.4 MB 1.9 MB/s eta 0:02:05\n",
      "   --------------- ------------------------ 151.8/382.4 MB 1.8 MB/s eta 0:02:06\n",
      "   --------------- ------------------------ 152.0/382.4 MB 1.8 MB/s eta 0:02:05\n",
      "   ---------------- ----------------------- 153.1/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 153.1/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 154.1/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 154.1/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 154.1/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 155.2/382.4 MB 1.9 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 156.2/382.4 MB 1.9 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 157.3/382.4 MB 1.9 MB/s eta 0:01:58\n",
      "   ---------------- ----------------------- 157.3/382.4 MB 1.9 MB/s eta 0:01:58\n",
      "   ---------------- ----------------------- 158.3/382.4 MB 1.9 MB/s eta 0:01:56\n",
      "   ---------------- ----------------------- 159.4/382.4 MB 2.0 MB/s eta 0:01:55\n",
      "   ---------------- ----------------------- 160.4/382.4 MB 2.0 MB/s eta 0:01:52\n",
      "   ---------------- ----------------------- 160.4/382.4 MB 2.0 MB/s eta 0:01:52\n",
      "   ---------------- ----------------------- 161.5/382.4 MB 2.0 MB/s eta 0:01:51\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 162.5/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 163.6/382.4 MB 2.0 MB/s eta 0:01:52\n",
      "   ----------------- ---------------------- 164.6/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 164.6/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 164.6/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 165.7/382.4 MB 2.0 MB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 167.8/382.4 MB 2.0 MB/s eta 0:01:50\n",
      "   ----------------- ---------------------- 168.8/382.4 MB 2.0 MB/s eta 0:01:49\n",
      "   ----------------- ---------------------- 168.8/382.4 MB 2.0 MB/s eta 0:01:49\n",
      "   ----------------- ---------------------- 169.9/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 169.9/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 169.9/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 170.1/382.4 MB 2.0 MB/s eta 0:01:48\n",
      "   ----------------- ---------------------- 171.4/382.4 MB 2.0 MB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 172.0/382.4 MB 2.0 MB/s eta 0:01:45\n",
      "   ------------------ --------------------- 172.8/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 173.0/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 173.0/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 173.0/382.4 MB 2.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 174.1/382.4 MB 2.0 MB/s eta 0:01:45\n",
      "   ------------------ --------------------- 175.1/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 175.1/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 176.2/382.4 MB 2.0 MB/s eta 0:01:44\n",
      "   ------------------ --------------------- 177.2/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 177.5/382.4 MB 2.0 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 178.3/382.4 MB 2.0 MB/s eta 0:01:41\n",
      "   ------------------ --------------------- 180.4/382.4 MB 2.1 MB/s eta 0:01:38\n",
      "   ------------------ --------------------- 181.4/382.4 MB 2.1 MB/s eta 0:01:36\n",
      "   ------------------ --------------------- 181.4/382.4 MB 2.1 MB/s eta 0:01:36\n",
      "   ------------------- -------------------- 182.2/382.4 MB 2.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 183.5/382.4 MB 2.2 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 184.3/382.4 MB 2.1 MB/s eta 0:01:34\n",
      "   ------------------- -------------------- 184.5/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 184.5/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 185.6/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 185.6/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 185.6/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 186.4/382.4 MB 2.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 186.6/382.4 MB 2.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 187.7/382.4 MB 2.2 MB/s eta 0:01:31\n",
      "   ------------------- -------------------- 187.7/382.4 MB 2.2 MB/s eta 0:01:31\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 188.7/382.4 MB 2.2 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 189.0/382.4 MB 2.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 190.8/382.4 MB 2.1 MB/s eta 0:01:30\n",
      "   ------------------- -------------------- 190.8/382.4 MB 2.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 191.6/382.4 MB 2.2 MB/s eta 0:01:28\n",
      "   -------------------- ------------------- 191.9/382.4 MB 2.2 MB/s eta 0:01:28\n",
      "   -------------------- ------------------- 192.7/382.4 MB 2.2 MB/s eta 0:01:27\n",
      "   -------------------- ------------------- 194.0/382.4 MB 2.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 194.0/382.4 MB 2.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 195.0/382.4 MB 2.3 MB/s eta 0:01:24\n",
      "   -------------------- ------------------- 195.0/382.4 MB 2.3 MB/s eta 0:01:24\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 197.1/382.4 MB 2.3 MB/s eta 0:01:22\n",
      "   -------------------- ------------------- 198.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 198.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 199.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 199.2/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 200.0/382.4 MB 2.1 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 200.3/382.4 MB 2.1 MB/s eta 0:01:26\n",
      "   --------------------- ------------------ 201.3/382.4 MB 2.2 MB/s eta 0:01:25\n",
      "   --------------------- ------------------ 202.1/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 202.4/382.4 MB 2.2 MB/s eta 0:01:24\n",
      "   --------------------- ------------------ 203.4/382.4 MB 2.2 MB/s eta 0:01:22\n",
      "   --------------------- ------------------ 204.5/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 204.5/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 205.5/382.4 MB 2.2 MB/s eta 0:01:19\n",
      "   --------------------- ------------------ 206.3/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 206.6/382.4 MB 2.2 MB/s eta 0:01:20\n",
      "   --------------------- ------------------ 207.6/382.4 MB 2.2 MB/s eta 0:01:19\n",
      "   --------------------- ------------------ 208.7/382.4 MB 2.3 MB/s eta 0:01:17\n",
      "   --------------------- ------------------ 208.7/382.4 MB 2.3 MB/s eta 0:01:17\n",
      "   --------------------- ------------------ 209.7/382.4 MB 2.3 MB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 210.8/382.4 MB 2.3 MB/s eta 0:01:15\n",
      "   ---------------------- ----------------- 211.8/382.4 MB 2.3 MB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 212.9/382.4 MB 2.3 MB/s eta 0:01:14\n",
      "   ---------------------- ----------------- 212.9/382.4 MB 2.3 MB/s eta 0:01:14\n",
      "   ---------------------- ----------------- 214.2/382.4 MB 2.3 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 215.0/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 215.7/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 216.0/382.4 MB 2.4 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 217.1/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 217.1/382.4 MB 2.3 MB/s eta 0:01:12\n",
      "   ---------------------- ----------------- 218.1/382.4 MB 2.3 MB/s eta 0:01:11\n",
      "   ---------------------- ----------------- 219.2/382.4 MB 2.4 MB/s eta 0:01:10\n",
      "   ----------------------- ---------------- 220.2/382.4 MB 2.4 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 220.2/382.4 MB 2.4 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 221.2/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 222.3/382.4 MB 2.4 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 223.3/382.4 MB 2.3 MB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 224.1/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 224.4/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 224.4/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 224.4/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 225.4/382.4 MB 2.3 MB/s eta 0:01:10\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 226.5/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 227.5/382.4 MB 2.2 MB/s eta 0:01:11\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ----------------------- ---------------- 228.6/382.4 MB 2.3 MB/s eta 0:01:09\n",
      "   ------------------------ --------------- 229.6/382.4 MB 2.2 MB/s eta 0:01:09\n",
      "   ------------------------ --------------- 230.7/382.4 MB 2.2 MB/s eta 0:01:09\n",
      "   ------------------------ --------------- 231.7/382.4 MB 2.3 MB/s eta 0:01:07\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 232.8/382.4 MB 2.3 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 233.8/382.4 MB 2.3 MB/s eta 0:01:05\n",
      "   ------------------------ --------------- 233.8/382.4 MB 2.3 MB/s eta 0:01:05\n",
      "   ------------------------ --------------- 234.9/382.4 MB 2.3 MB/s eta 0:01:04\n",
      "   ------------------------ --------------- 234.9/382.4 MB 2.3 MB/s eta 0:01:04\n",
      "   ------------------------ --------------- 234.9/382.4 MB 2.3 MB/s eta 0:01:04\n",
      "   ------------------------ --------------- 235.9/382.4 MB 2.2 MB/s eta 0:01:06\n",
      "   ------------------------ --------------- 237.0/382.4 MB 2.3 MB/s eta 0:01:05\n",
      "   ------------------------ --------------- 237.8/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------ --------------- 238.0/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 239.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 239.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 240.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 240.1/382.4 MB 2.3 MB/s eta 0:01:03\n",
      "   ------------------------- -------------- 241.2/382.4 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------------------- -------------- 241.2/382.4 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 242.2/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 243.3/382.4 MB 2.3 MB/s eta 0:01:02\n",
      "   ------------------------- -------------- 244.3/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 245.4/382.4 MB 2.3 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 246.4/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 246.4/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 247.5/382.4 MB 2.2 MB/s eta 0:01:01\n",
      "   ------------------------- -------------- 248.3/382.4 MB 2.2 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 248.5/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   ------------------------- -------------- 248.5/382.4 MB 2.3 MB/s eta 0:01:00\n",
      "   -------------------------- ------------- 248.8/382.4 MB 2.2 MB/s eta 0:01:01\n",
      "   -------------------------- ------------- 250.3/382.4 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------------------- ------------- 250.6/382.4 MB 2.3 MB/s eta 0:00:59\n",
      "   -------------------------- ------------- 251.7/382.4 MB 2.3 MB/s eta 0:00:58\n",
      "   -------------------------- ------------- 252.4/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 252.7/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 252.7/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 252.7/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 253.8/382.4 MB 2.3 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 254.8/382.4 MB 2.2 MB/s eta 0:00:57\n",
      "   -------------------------- ------------- 255.6/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 255.9/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   -------------------------- ------------- 256.9/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   -------------------------- ------------- 257.9/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 258.5/382.4 MB 2.3 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 259.5/382.4 MB 2.3 MB/s eta 0:00:55\n",
      "   --------------------------- ------------ 260.0/382.4 MB 2.3 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 260.0/382.4 MB 2.3 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 261.1/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 261.9/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 262.1/382.4 MB 2.3 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 263.2/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 263.2/382.4 MB 2.2 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 264.2/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 265.3/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 266.3/382.4 MB 2.3 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 267.4/382.4 MB 2.3 MB/s eta 0:00:51\n",
      "   --------------------------- ------------ 267.4/382.4 MB 2.3 MB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 268.2/382.4 MB 2.3 MB/s eta 0:00:50\n",
      "   ---------------------------- ----------- 269.5/382.4 MB 2.3 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 269.5/382.4 MB 2.3 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 269.5/382.4 MB 2.3 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 270.5/382.4 MB 2.3 MB/s eta 0:00:48\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 271.6/382.4 MB 2.4 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 272.6/382.4 MB 2.3 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 273.4/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 273.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 273.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 274.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 274.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 274.7/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ---------------------------- ----------- 275.8/382.4 MB 2.1 MB/s eta 0:00:50\n",
      "   ---------------------------- ----------- 276.8/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 277.9/382.4 MB 2.2 MB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 278.9/382.4 MB 2.2 MB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 278.9/382.4 MB 2.2 MB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 280.2/382.4 MB 2.1 MB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 281.0/382.4 MB 2.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 283.1/382.4 MB 2.2 MB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 284.2/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 285.0/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 285.2/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 286.3/382.4 MB 2.2 MB/s eta 0:00:44\n",
      "   ------------------------------ --------- 287.3/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 287.3/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 288.4/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 288.4/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 288.4/382.4 MB 2.2 MB/s eta 0:00:43\n",
      "   ------------------------------ --------- 289.4/382.4 MB 2.2 MB/s eta 0:00:42\n",
      "   ------------------------------ --------- 290.5/382.4 MB 2.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 291.5/382.4 MB 2.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 291.5/382.4 MB 2.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 292.6/382.4 MB 2.3 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 293.6/382.4 MB 2.3 MB/s eta 0:00:38\n",
      "   ------------------------------ --------- 294.1/382.4 MB 2.4 MB/s eta 0:00:38\n",
      "   ------------------------------ --------- 294.6/382.4 MB 2.4 MB/s eta 0:00:37\n",
      "   ------------------------------ --------- 295.4/382.4 MB 2.4 MB/s eta 0:00:37\n",
      "   ------------------------------- -------- 296.7/382.4 MB 2.4 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 297.8/382.4 MB 2.4 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 297.8/382.4 MB 2.4 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 298.8/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 299.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 300.9/382.4 MB 2.5 MB/s eta 0:00:34\n",
      "   ------------------------------- -------- 302.0/382.4 MB 2.5 MB/s eta 0:00:33\n",
      "   ------------------------------- -------- 303.0/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 303.0/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 303.0/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 304.9/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   ------------------------------- -------- 305.1/382.4 MB 2.4 MB/s eta 0:00:32\n",
      "   -------------------------------- ------- 306.2/382.4 MB 2.5 MB/s eta 0:00:32\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 307.2/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 308.3/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 308.3/382.4 MB 2.5 MB/s eta 0:00:31\n",
      "   -------------------------------- ------- 310.4/382.4 MB 2.5 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 310.4/382.4 MB 2.5 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 311.4/382.4 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 312.5/382.4 MB 2.4 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 312.5/382.4 MB 2.4 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 312.5/382.4 MB 2.4 MB/s eta 0:00:30\n",
      "   -------------------------------- ------- 313.5/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 314.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 314.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   -------------------------------- ------- 314.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 315.6/382.4 MB 2.4 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 316.7/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 316.7/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 317.7/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 318.2/382.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------------------------- ------ 318.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 319.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 319.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 319.8/382.4 MB 2.5 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 320.6/382.4 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 320.9/382.4 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------------------------- ------ 321.9/382.4 MB 2.5 MB/s eta 0:00:25\n",
      "   --------------------------------- ------ 321.9/382.4 MB 2.5 MB/s eta 0:00:25\n",
      "   --------------------------------- ------ 323.0/382.4 MB 2.4 MB/s eta 0:00:25\n",
      "   --------------------------------- ------ 324.0/382.4 MB 2.5 MB/s eta 0:00:24\n",
      "   --------------------------------- ------ 324.0/382.4 MB 2.5 MB/s eta 0:00:24\n",
      "   ---------------------------------- ----- 325.1/382.4 MB 2.4 MB/s eta 0:00:24\n",
      "   ---------------------------------- ----- 326.1/382.4 MB 2.5 MB/s eta 0:00:23\n",
      "   ---------------------------------- ----- 326.1/382.4 MB 2.5 MB/s eta 0:00:23\n",
      "   ---------------------------------- ----- 327.2/382.4 MB 2.5 MB/s eta 0:00:23\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 328.2/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 329.8/382.4 MB 2.5 MB/s eta 0:00:22\n",
      "   ---------------------------------- ----- 330.3/382.4 MB 2.5 MB/s eta 0:00:21\n",
      "   ---------------------------------- ----- 331.4/382.4 MB 2.5 MB/s eta 0:00:21\n",
      "   ---------------------------------- ----- 332.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ---------------------------------- ----- 333.4/382.4 MB 2.5 MB/s eta 0:00:20\n",
      "   ----------------------------------- ---- 335.5/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 336.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.4/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 337.6/382.4 MB 2.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 338.7/382.4 MB 2.5 MB/s eta 0:00:18\n",
      "   ----------------------------------- ---- 339.7/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 339.7/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 339.7/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 340.8/382.4 MB 2.5 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.6/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.8/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.8/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 341.8/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 342.4/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 342.9/382.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ----------------------------------- ---- 343.9/382.4 MB 2.4 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 344.5/382.4 MB 2.5 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 345.8/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 346.0/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 347.1/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 347.6/382.4 MB 2.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 348.1/382.4 MB 2.5 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 348.1/382.4 MB 2.5 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 348.9/382.4 MB 2.4 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 349.2/382.4 MB 2.4 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 350.2/382.4 MB 2.4 MB/s eta 0:00:14\n",
      "   ------------------------------------ --- 351.0/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 352.3/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 352.3/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 352.3/382.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 353.4/382.4 MB 2.5 MB/s eta 0:00:12\n",
      "   ------------------------------------ --- 353.4/382.4 MB 2.5 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 353.9/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 354.4/382.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 355.5/382.4 MB 2.3 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 356.0/382.4 MB 2.3 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 356.5/382.4 MB 2.3 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 357.3/382.4 MB 2.3 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 358.6/382.4 MB 2.3 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 358.6/382.4 MB 2.3 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 359.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 360.7/382.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 362.3/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 362.8/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 363.9/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 363.9/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 363.9/382.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 364.9/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 364.9/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 365.7/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 366.0/382.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 367.0/382.4 MB 2.3 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 367.0/382.4 MB 2.3 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 367.0/382.4 MB 2.3 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 367.8/382.4 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 368.1/382.4 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 369.1/382.4 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 370.1/382.4 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 371.2/382.4 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 371.2/382.4 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 372.2/382.4 MB 2.3 MB/s eta 0:00:05\n",
      "   ---------------------------------------  373.3/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  373.3/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  374.3/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  375.1/382.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------------------------------  376.2/382.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------------------  376.4/382.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------------------  377.5/382.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  378.5/382.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  379.6/382.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  379.6/382.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  380.4/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.7/382.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.2/382.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 382.4/382.4 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 2.9/4.7 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.7/4.7 MB 12.4 MB/s eta 0:00:00\n",
      "Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 2.4/2.9 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 3.1/26.4 MB 15.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.0/26.4 MB 14.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.7/26.4 MB 13.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.5/26.4 MB 13.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.9/26.4 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.8/26.4 MB 13.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.2/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.6/15.5 MB 15.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.2/15.5 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.4/15.5 MB 14.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.1/15.5 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 4.2/5.5 MB 20.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-win_amd64.whl (314 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.31.1\n",
      "    Uninstalling protobuf-6.31.1:\n",
      "      Successfully uninstalled protobuf-6.31.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.11.3 libclang-18.1.1 markdown-3.9 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.1.0 numpy-1.26.4 opt-einsum-3.4.0 optree-0.17.0 protobuf-4.25.8 rich-14.2.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.1 tensorflow-intel-2.17.1 termcolor-3.2.0 werkzeug-3.1.3 wrapt-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\parid\\anaconda3\\envs\\Rohit\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\parid\\anaconda3\\envs\\Rohit\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.9.2 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\parid\\anaconda3\\envs\\rohit\\lib\\site-packages (from matplotlib==3.9.2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\parid\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "==== All required libraries are installed =====\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.1\n",
    "!pip install matplotlib==3.9.2\n",
    "\n",
    "print(\"==== All required libraries are installed =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress the tensorflow warning messages\n",
    "We use the following code to  suppress the warning messages due to use of CPU architechture for tensoflow.\n",
    "\n",
    "You may want to **comment out** these lines if you are using the GPU architechture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To use Keras, you will also need to install a backend framework  such as TensorFlow.\n",
    "\n",
    "If you install TensorFlow 2.16 or above, it will install Keras by default.\n",
    "\n",
    "We are using the CPU version of tensorflow since we are dealing with smaller datasets. \n",
    "You may install the GPU version of tensorflow on your machine to accelarate the processing of larger datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "We start by define the sentences and text for translation training\n",
    "Sentence Pairs: Defines a small dataset of English-Spanish sentence pairs.\n",
    "Target Sequences:\n",
    "Prepends \"startseq\" and appends \"endseq\" to each target sentence for the decoder to learn when to start and stop translating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parallel sentences (English -> Spanish)\n",
    "input_texts = [\n",
    "    \"Hello.\", \"How are you?\", \"I am learning machine translation.\", \"What is your name?\", \"I love programming.\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hola.\", \"Cmo ests?\", \"Estoy aprendiendo traduccin automtica.\", \"Cul es tu nombre?\", \"Me encanta programar.\"\n",
    "]\n",
    "\n",
    "target_texts = [\"startseq \" + x + \" endseq\" for x in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we convert the text from the sentences to tokens and create a vocabulary\n",
    "Tokenization: Uses Tokenizer to convert words into numerical sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [3, 4, 5], [1, 6, 7, 8, 9], [10, 11, 12, 13], [1, 14, 15]]\n",
      "[[1, 3, 2], [1, 4, 5, 2], [1, 6, 7, 8, 9, 2], [1, 10, 11, 12, 13, 2], [1, 14, 15, 16, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "# print(input_tokenizer)\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "print(input_sequences)\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "print(output_sequences)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 14\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pad the corresponding sentences\n",
    "Padding: Ensures all sequences have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "max_input_length = max([len(seq) for seq in input_sequences])\n",
    "max_output_length = max([len(seq) for seq in output_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target data for training\n",
    "decoder_input_data = output_sequences[:, :-1]\n",
    "decoder_output_data = output_sequences[:, 1:]\n",
    "\n",
    "# Convert to one-hot\n",
    "decoder_output_data = np.array([np.eye(output_vocab_size)[seq] for seq in decoder_output_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Self-Attention Layer\n",
    "Self-attention is a mechanism that allows a model to **focus on relevant parts of the input sequence** while processing each word. This is particularly useful in:\n",
    "1) Machine Translation (e.g., aligning words correctly)\n",
    "2) Text Summarization\n",
    "3) Speech Recognition\n",
    "4) Image Processing (Vision Transformers)\n",
    "In this implementation, self-attention is used for text based sequence-to-sequence modeling.\n",
    "\n",
    "\n",
    "Self-Attention works for a given an input sequence by computing a weighted representation of all words for each position. It does so using three key components:\n",
    "\n",
    "1. Query **(Q)**, Key **(K)**, and Value **(V)** Matrices\n",
    "For each word (token) in a sequence:\n",
    "\n",
    "Query (Q): What this word is looking for.\n",
    "Key (K): What this word represents.\n",
    "Value (V): The actual information in the word.\n",
    "\n",
    "2. Compute **Attention Scores**\n",
    "Next, we **calculate the similarity between each query and key** using dot-product attention:\n",
    "Each word in a sequence attends to every other word based on these scores.\n",
    "\n",
    "3. Apply **Scaling & Softmax**\n",
    "Since dot-product values can be large, we scale them. \n",
    "Next, Applying softmax converts scores into attention weights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention class\n",
    "In this implementation of self-attention layer:\n",
    "1. We first initialize the weights in the **build** method, where:\n",
    "    1. **self.Wq**, **self.Wk**, **self.Wv** are the trainable weight matrices.\n",
    "    2. Their **shape is (feature_dim, feature_dim)**, meaning they transform input features into Q, K, and V representations.\n",
    "2. Applying Attention using **call** method. The **call()** method:\n",
    "   1. Computes **Q, K, V** by multiplying inputs (encoder/decoder output) with their respective weight matrices.\n",
    "   2. Computes **dot-product attention scores** using K.batch_dot(q, k, axes=[2, 2]), resulting in a (batch_size, seq_len, seq_len) matrix.\n",
    "   3. **Scales** the scores to avoid large values.\n",
    "   4. Applies **softmax** to normalize the attention scores.\n",
    "   5. **Multiplies attention weights with V** to get the final output.\n",
    "3. The **compute_output_shape** method defines the shape of the output tensor after the layer processes an input.\n",
    "    1. The output shape of the Self-Attention layer **remains the same** as the input shape.\n",
    "    2. The attention mechanism **transforms** the input but does not change its dimensions.4\n",
    "    3. If the attention layer changed the shape, you would modify compute_output_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Architecture\n",
    "The model follows an Encoder-Decoder structure:\n",
    "\n",
    "### Encoder:\n",
    "1) Takes input sentences (padded and tokenized).\n",
    "2) Uses an Embedding layer (word representations) + LSTM (to process sequences).\n",
    "    1. The LSTMs are used as the **help process variable-length input sentences** and generate meaningful translations.\n",
    "4) Outputs context vectors (hidden & cell states).\n",
    "\n",
    "### Attention Layer\n",
    "1) Applied to both the encoder and decoder outputs.\n",
    "2) Helps the decoder focus on relevant words during translation.\n",
    "\n",
    "### Decoder\n",
    "1) Receives target sequences (shifted one step ahead).\n",
    "2) Uses an LSTM with encoder states as initial states.\n",
    "3) Applies self-attention for better learning.\n",
    "4) Uses a Dense layer (Softmax) to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " input_layer          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " input_layer_1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " embedding            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " embedding_1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
       "\n",
       " lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span>  embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),                                     \n",
       "                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                                     \n",
       "\n",
       " lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)        [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span>  embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),                   lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       \n",
       "                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                   lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        \n",
       "\n",
       " additive_attention   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>)                                 lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " concatenate          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       additive_attenti \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,721</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " input_layer_1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " embedding            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)          \u001b[38;5;34m7,424\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " embedding_1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)          \u001b[38;5;34m4,352\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
       " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
       "\n",
       " lstm (\u001b[38;5;33mLSTM\u001b[0m)          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),      \u001b[38;5;34m525,312\u001b[0m  embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),                                     \n",
       "                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]                                     \n",
       "\n",
       " lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)        [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),      \u001b[38;5;34m525,312\u001b[0m  embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m \n",
       "                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),                   lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       \n",
       "                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]                   lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        \n",
       "\n",
       " additive_attention   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m256\u001b[0m  lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
       " (\u001b[38;5;33mAdditiveAttention\u001b[0m)                                 lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " concatenate          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m0\u001b[0m  lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
       " (\u001b[38;5;33mConcatenate\u001b[0m)                                       additive_attenti \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m17\u001b[0m)           \u001b[38;5;34m8,721\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,377</span> (4.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,071,377\u001b[0m (4.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,377</span> (4.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,071,377\u001b[0m (4.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, Dense, Embedding, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    " \n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    " \n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    " \n",
    "# Attention: decoder attends to encoder outputs\n",
    "attention = AdditiveAttention()\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    " \n",
    "# Combine decoder outputs with attention context\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    " \n",
    "# Final Dense layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    " \n",
    "# Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training the Model\n",
    "Uses categorical_crossentropy as the loss function since output words are one-hot encoded.\n",
    "Trains using Adam optimizer for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.0400 - loss: 2.8374\n",
      "Epoch 2/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.3200 - loss: 2.8061\n",
      "Epoch 3/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3200 - loss: 2.7733\n",
      "Epoch 4/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.3200 - loss: 2.7360\n",
      "Epoch 5/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3200 - loss: 2.6909\n",
      "Epoch 6/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.3200 - loss: 2.6335\n",
      "Epoch 7/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.3200 - loss: 2.5584\n",
      "Epoch 8/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.2400 - loss: 2.4591\n",
      "Epoch 9/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.2400 - loss: 2.3323\n",
      "Epoch 10/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.2400 - loss: 2.1913\n",
      "Epoch 11/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.2400 - loss: 2.0871\n",
      "Epoch 12/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2400 - loss: 2.0690\n",
      "Epoch 13/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2400 - loss: 2.0567\n",
      "Epoch 14/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.2400 - loss: 1.9956\n",
      "Epoch 15/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.3200 - loss: 1.9214\n",
      "Epoch 16/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4400 - loss: 1.8709\n",
      "Epoch 17/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.4000 - loss: 1.8300\n",
      "Epoch 18/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.4000 - loss: 1.7786\n",
      "Epoch 19/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.4800 - loss: 1.7193\n",
      "Epoch 20/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.5200 - loss: 1.6592\n",
      "Epoch 21/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.5600 - loss: 1.6018\n",
      "Epoch 22/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5600 - loss: 1.5471\n",
      "Epoch 23/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.5600 - loss: 1.4942\n",
      "Epoch 24/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5200 - loss: 1.4413\n",
      "Epoch 25/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.4400 - loss: 1.3856\n",
      "Epoch 26/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.4400 - loss: 1.3237\n",
      "Epoch 27/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.4800 - loss: 1.2528\n",
      "Epoch 28/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.5600 - loss: 1.1739\n",
      "Epoch 29/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 0.6400 - loss: 1.0938\n",
      "Epoch 30/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.7200 - loss: 1.0193\n",
      "Epoch 31/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8000 - loss: 0.9512\n",
      "Epoch 32/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8400 - loss: 0.8850\n",
      "Epoch 33/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8400 - loss: 0.8197\n",
      "Epoch 34/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8800 - loss: 0.7576\n",
      "Epoch 35/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8800 - loss: 0.6978\n",
      "Epoch 36/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9200 - loss: 0.6392\n",
      "Epoch 37/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.5839\n",
      "Epoch 38/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.5344\n",
      "Epoch 39/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.4899\n",
      "Epoch 40/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.4492\n",
      "Epoch 41/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.4128\n",
      "Epoch 42/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.3785\n",
      "Epoch 43/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.3466\n",
      "Epoch 44/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.3183\n",
      "Epoch 45/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.2918\n",
      "Epoch 46/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.2686\n",
      "Epoch 47/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.2481\n",
      "Epoch 48/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.2283\n",
      "Epoch 49/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.2099\n",
      "Epoch 50/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.1926\n",
      "Epoch 51/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.1774\n",
      "Epoch 52/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.1630\n",
      "Epoch 53/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.1491\n",
      "Epoch 54/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.1370\n",
      "Epoch 55/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.1257\n",
      "Epoch 56/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.1155\n",
      "Epoch 57/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.1060\n",
      "Epoch 58/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 0.0976\n",
      "Epoch 59/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0900\n",
      "Epoch 60/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 0.0829\n",
      "Epoch 61/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 1.0000 - loss: 0.0768\n",
      "Epoch 62/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0709\n",
      "Epoch 63/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0654\n",
      "Epoch 64/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0604\n",
      "Epoch 65/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0559\n",
      "Epoch 66/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0519\n",
      "Epoch 67/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0481\n",
      "Epoch 68/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0448\n",
      "Epoch 69/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0417\n",
      "Epoch 70/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 1.0000 - loss: 0.0389\n",
      "Epoch 71/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0363\n",
      "Epoch 72/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0338\n",
      "Epoch 73/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0317\n",
      "Epoch 74/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0296\n",
      "Epoch 75/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0278\n",
      "Epoch 76/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 0.0262\n",
      "Epoch 77/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0246\n",
      "Epoch 78/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0233\n",
      "Epoch 79/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0220\n",
      "Epoch 80/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0208\n",
      "Epoch 81/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0197\n",
      "Epoch 82/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0187\n",
      "Epoch 83/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0178\n",
      "Epoch 84/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0169\n",
      "Epoch 85/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0161\n",
      "Epoch 86/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0154\n",
      "Epoch 87/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0147\n",
      "Epoch 88/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.0141\n",
      "Epoch 89/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0135\n",
      "Epoch 90/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.0130\n",
      "Epoch 91/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 1.0000 - loss: 0.0124\n",
      "Epoch 92/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 0.0120\n",
      "Epoch 93/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0115\n",
      "Epoch 94/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 1.0000 - loss: 0.0111\n",
      "Epoch 95/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 1.0000 - loss: 0.0107\n",
      "Epoch 96/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0103\n",
      "Epoch 97/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0100\n",
      "Epoch 98/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0096\n",
      "Epoch 99/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0093\n",
      "Epoch 100/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0090\n",
      "Epoch 101/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0088\n",
      "Epoch 102/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0085\n",
      "Epoch 103/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0082\n",
      "Epoch 104/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0080\n",
      "Epoch 105/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0078\n",
      "Epoch 106/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0076\n",
      "Epoch 107/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0074\n",
      "Epoch 108/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.0072\n",
      "Epoch 109/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0070\n",
      "Epoch 110/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0068\n",
      "Epoch 111/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0067\n",
      "Epoch 112/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0065\n",
      "Epoch 113/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0064\n",
      "Epoch 114/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0062\n",
      "Epoch 115/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 1.0000 - loss: 0.0061\n",
      "Epoch 116/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0059\n",
      "Epoch 117/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0058\n",
      "Epoch 118/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0057\n",
      "Epoch 119/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0056\n",
      "Epoch 120/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0055\n",
      "Epoch 121/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0053\n",
      "Epoch 122/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0052\n",
      "Epoch 123/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0051\n",
      "Epoch 124/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0050\n",
      "Epoch 125/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "Epoch 126/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "Epoch 127/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0048\n",
      "Epoch 128/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0047\n",
      "Epoch 129/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0046\n",
      "Epoch 130/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 131/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 132/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 133/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0043\n",
      "Epoch 134/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0042\n",
      "Epoch 135/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 136/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 137/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0040\n",
      "Epoch 138/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0040\n",
      "Epoch 139/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0039\n",
      "Epoch 140/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 141/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 142/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0037\n",
      "Epoch 143/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0037\n",
      "Epoch 144/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0036\n",
      "Epoch 145/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0036\n",
      "Epoch 146/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 147/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 148/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Epoch 149/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Epoch 150/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 151/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 152/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 153/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 154/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 155/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 156/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 157/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 158/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 159/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0029\n",
      "Epoch 160/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0029\n",
      "Epoch 161/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0029\n",
      "Epoch 162/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 163/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 164/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 165/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 166/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 167/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 168/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 169/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 170/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 171/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 172/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 173/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 174/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 175/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 176/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 177/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 178/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 179/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 180/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 181/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 182/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 183/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 184/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 185/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 186/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 187/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 188/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 189/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 190/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 191/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 192/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 193/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 194/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 195/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 196/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 197/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 198/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 199/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 200/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 201/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 202/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 203/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 204/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 205/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 206/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 207/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 208/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 209/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 210/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 211/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 212/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 213/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 214/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 215/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 216/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 217/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 218/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 219/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 220/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 221/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 222/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 223/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 224/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 225/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 226/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 227/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 228/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 229/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 230/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 231/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 232/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 233/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 234/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 235/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 236/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 237/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 238/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 239/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 240/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 241/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 242/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 243/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 244/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 245/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 246/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 247/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 248/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 249/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 250/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 251/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 252/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 253/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 254/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 255/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 256/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 257/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 258/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 259/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 260/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 261/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 262/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 263/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 264/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 265/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 266/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 267/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 268/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 269/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 270/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 271/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 272/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 273/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 274/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 275/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 276/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 277/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 9.9826e-04\n",
      "Epoch 278/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 9.9153e-04\n",
      "Epoch 279/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 9.8487e-04\n",
      "Epoch 280/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 9.7829e-04\n",
      "Epoch 281/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 1.0000 - loss: 9.7181e-04\n",
      "Epoch 282/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 9.6536e-04\n",
      "Epoch 283/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 9.5898e-04\n",
      "Epoch 284/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 9.5267e-04\n",
      "Epoch 285/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 9.4642e-04\n",
      "Epoch 286/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 9.4025e-04\n",
      "Epoch 287/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 9.3411e-04\n",
      "Epoch 288/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 9.2808e-04\n",
      "Epoch 289/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 9.2209e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 9.1615e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 9.1029e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 9.0450e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 8.9874e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 8.9306e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 8.8745e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 8.8184e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 8.7633e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 8.7087e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 8.6546e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 8.6011e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "history_glorot_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=300, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plotting the training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome, now you have succesfully trained a transformers model.\n",
    "### Now let's try some practice excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, let's train the model using \"he_uniform\" initializer instead of \"glorot_uniform\". Then, compare the training loss between model using \"glorot_uniform\" vs \"he_uniform\" initializers by plotting them using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.1200 - loss: 2.8313\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.3200 - loss: 2.7994\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.3200 - loss: 2.7653\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3200 - loss: 2.7257\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.3200 - loss: 2.6769\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.2800 - loss: 2.6137\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2800 - loss: 2.5299\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.2400 - loss: 2.4190\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.2400 - loss: 2.2805\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.2400 - loss: 2.1399\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.2400 - loss: 2.0620\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.2400 - loss: 2.0643\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.2400 - loss: 2.0426\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.2400 - loss: 1.9734\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.3200 - loss: 1.8992\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.4400 - loss: 1.8477\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.4400 - loss: 1.8056\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4800 - loss: 1.7574\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.5600 - loss: 1.7027\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.6400 - loss: 1.6452\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.6400 - loss: 1.5880\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.5600 - loss: 1.5329\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.5600 - loss: 1.4795\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.6000 - loss: 1.4256\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4800 - loss: 1.3671\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.4800 - loss: 1.3013\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.4800 - loss: 1.2287\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5200 - loss: 1.1507\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.6000 - loss: 1.0719\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.6800 - loss: 1.0029\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7200 - loss: 0.9471\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7200 - loss: 0.8937\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.7200 - loss: 0.8387\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.7600 - loss: 0.7862\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8000 - loss: 0.7334\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9200 - loss: 0.6806\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9600 - loss: 0.6298\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.5824\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.5420\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.5021\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.4606\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 1.0000 - loss: 0.4204\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.3846\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.3538\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.3240\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.2976\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 1.0000 - loss: 0.2751\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.2523\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 1.0000 - loss: 0.2308\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.2138\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.1973\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.1815\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 0.1672\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 1.0000 - loss: 0.1542\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.1411\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.1298\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.1206\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.1113\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.1023\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 1.0000 - loss: 0.0943\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0871\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0803\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 1.0000 - loss: 0.0745\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 1.0000 - loss: 0.0692\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 0.0639\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0591\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0550\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0512\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0477\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0444\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0415\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0388\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0362\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 0.0340\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 0.0319\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0301\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0284\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0267\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0252\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 1.0000 - loss: 0.0239\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0226\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0214\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0204\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0194\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0185\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0176\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0168\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0161\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0154\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0147\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0141\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0136\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0130\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0125\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0121\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.0117\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 1.0000 - loss: 0.0113\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.0109\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 0.0105\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 1.0000 - loss: 0.0102\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write your answer here\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "\n",
    "#Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(output_vocab_size, 256, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "#Attention Mechanism\n",
    "attention = AdditiveAttention(name=\"attention_layer\")\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context with decoder outputs\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")([decoder_outputs, attention_output])\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, try to use adaptive gradient optimizer instead of adam. Then, plot and compare the results between adam and adaptive gradient optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0098\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0098\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Write your answer here\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n",
    "plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n",
    "plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2024-11-20  | 1.0  | Aman  |  Created the lab |\n",
    "<hr>\n",
    "-->\n",
    "## <h3 align=\"center\">  IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rohit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "3a07fb4049049613c9f3bf3a0aaeeac466433593dd808e2778bab531403fe8a9"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
