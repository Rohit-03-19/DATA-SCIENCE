{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc9e485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Triggering a 'Save' every 100 steps\n",
    "step = 450\n",
    "if step % 100 == 0:\n",
    "    print(\"Saving checkpoint...\")\n",
    "\n",
    "# Calculating how many full batches we have\n",
    "full_batches = 1000 // 32 # Result: 31\n",
    "full_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00b2d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 0.30000000000000004\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 1. Using Scientific Notation for Hyperparameters\n",
    "learning_rate = 1e-4  # 0.0001\n",
    "weight_decay = 5e-5   # 0.00005\n",
    "\n",
    "# 2. The Floating Point Trap\n",
    "loss1 = 0.1\n",
    "loss2 = 0.2\n",
    "total_loss = loss1 + loss2\n",
    "\n",
    "print(f\"Total Loss: {total_loss}\") # Result: 0.30000000000000004\n",
    "# Senior Tip: Never use '==' with floats in production. Use math.isclose()\n",
    "import math\n",
    "print(math.isclose(total_loss, 0.3)) # Result: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658b0bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successes: 3 | Rate: 60.00%\n"
     ]
    }
   ],
   "source": [
    "# A list representing if 5 model inferences were successful\n",
    "results = [True, False, True, True, False]\n",
    "\n",
    "# How many succeeded? \n",
    "# sum() converts True to 1 and False to 0 automatically\n",
    "success_count = sum(results) \n",
    "\n",
    "# Calculate success rate\n",
    "success_rate = sum(results) / len(results)\n",
    "\n",
    "print(f\"Successes: {success_count} | Rate: {success_rate:.2%}\")\n",
    "# Result: Successes: 3 | Rate: 60.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8acabbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Hello world!\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"  Input: Hello World!  \"\n",
    "\n",
    "# Strip whitespace and convert to lowercase\n",
    "clean_text = raw_text.strip().lower() # \"input: hello world!\"\n",
    "\n",
    "# Check if it starts with a keyword\n",
    "if clean_text.startswith(\"input:\"):\n",
    "    content = clean_text.replace(\"input:\", \"\").strip()\n",
    "    print(f\"Content: {content.capitalize()}\") # \"hello world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f7715b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an AI Assistant working on the Safety-Guard project.\n",
      "Your response limit is exactly 512 tokens.\n",
      "Do not mention external links.\n",
      "\n",
      "Deploying v1 (Beta: True)\n"
     ]
    }
   ],
   "source": [
    "# 1. System Prompt for an LLM\n",
    "project_name = \"Safety-Guard\"\n",
    "max_tokens = 512\n",
    "\n",
    "system_message = f\"\"\"\n",
    "You are an AI Assistant working on the {project_name} project.\n",
    "Your response limit is exactly {max_tokens} tokens.\n",
    "Do not mention external links.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Advanced Slicing (Reversing or extracting parts)\n",
    "# Extracting the last 3 characters of a version string\n",
    "version = \"v1.4.2b\"\n",
    "suffix = version[-1] # 'b'\n",
    "major_version = version[:2] # 'v1'\n",
    "\n",
    "print(system_message)\n",
    "print(f\"Deploying {major_version} (Beta: {suffix == 'b'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470a4da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/path/to/weights.pt\n"
     ]
    }
   ],
   "source": [
    "# Using tuples as a 'Key' for model settings\n",
    "model_registry = {}\n",
    "\n",
    "# Key is (architecture, version)\n",
    "config_key = (\"ResNet50\", \"v1.2\")\n",
    "model_registry[config_key] = \"/path/to/weights.pt\"\n",
    "\n",
    "print(model_registry[(\"ResNet50\", \"v1.2\")]) # Fast and safe lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79109b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = {\n",
    "    \"id\": \"chat-123\",\n",
    "    \"choices\": [\n",
    "        {\"text\": \"Hello!\", \"index\": 0, \"finish_reason\": \"stop\"}\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 5, \"completion_tokens\": 2}\n",
    "}\n",
    "\n",
    "# Accessing nested data safely\n",
    "tokens = response.get(\"usage\", {}).get(\"prompt_tokens\", 0) \n",
    "tokens\n",
    "# Senior Tip: Always use .get() to avoid crashing on missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce3fc3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Leakage Found! IDs: {105}\n",
      "Cleaned Train IDs: {104, 101, 102, 103}\n"
     ]
    }
   ],
   "source": [
    "train_ids = {101, 102, 103, 104, 105}\n",
    "test_ids = {105, 106, 107}\n",
    "\n",
    "# Find overlapping IDs using the Intersection operator (&)\n",
    "overlap = train_ids & test_ids\n",
    "\n",
    "if overlap:\n",
    "    print(f\"Data Leakage Found! IDs: {overlap}\")\n",
    "    # Clean the training set using Difference operator (-)\n",
    "    clean_train = train_ids - overlap\n",
    "    print(f\"Cleaned Train IDs: {clean_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69e1dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decayed Learning Rate: 0.059874\n"
     ]
    }
   ],
   "source": [
    "initial_lr = 0.1\n",
    "decay_rate = 0.95\n",
    "global_step = 10 # 10th epoch or step\n",
    "\n",
    "# New LR = initial_lr * (decay_rate ^ global_step)\n",
    "current_lr = initial_lr * (decay_rate ** global_step)\n",
    "print(f\"Decayed Learning Rate: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac098aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 passed the filter.\n",
      "Sample 2 passed the filter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parid\\AppData\\Local\\Temp\\ipykernel_14348\\2839114049.py:8: DeprecationWarning: Bitwise inversion '~' on bool is deprecated and will be removed in Python 3.16. This returns the bitwise inversion of the underlying int object and is usually not what you expect from negating a bool. Use the 'not' operator for boolean negation or ~int(x) if you really want the bitwise inversion of the underlying int.\n",
      "  mask = (confidence_scores[i] > 0.9) & (~is_duplicate[i])\n"
     ]
    }
   ],
   "source": [
    "# Sample scores and flags\n",
    "confidence_scores = [0.95, 0.88, 0.99, 0.45]\n",
    "is_duplicate = [False, True, False, False]\n",
    "\n",
    "# Senior AIOps logic: Generate a mask\n",
    "# We want confidence > 0.9 AND NOT duplicate\n",
    "for i in range(len(confidence_scores)):\n",
    "    mask = (confidence_scores[i] > 0.9) & (~is_duplicate[i])\n",
    "    if mask:\n",
    "        print(f\"Sample {i} passed the filter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb8f43ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss improved! Saving weights...\n"
     ]
    }
   ],
   "source": [
    "best_loss = 0.15\n",
    "current_loss = 0.12\n",
    "patience_counter = 0\n",
    "\n",
    "if current_loss < best_loss:\n",
    "    print(\"Loss improved! Saving weights...\")\n",
    "    best_loss = current_loss\n",
    "    patience_counter = 0 # Reset patience\n",
    "else:\n",
    "    patience_counter += 1\n",
    "    print(f\"No improvement. Patience: {patience_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc2a8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Gradient for update: 0.1\n"
     ]
    }
   ],
   "source": [
    "total_gradient = 0\n",
    "accumulation_steps = 4\n",
    "\n",
    "for i in range(accumulation_steps):\n",
    "    grad_step = 0.025 # Simulated gradient from a small batch\n",
    "    total_gradient += grad_step # Accumulate\n",
    "\n",
    "# Finally, update using the accumulated total\n",
    "print(f\"Total Gradient for update: {total_gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a1696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rohit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
