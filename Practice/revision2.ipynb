{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b84f52b",
   "metadata": {},
   "source": [
    "### Revision of Python Intermediate level and Above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfdfc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Iris-Classifier built with Scikit-Learn\n"
     ]
    }
   ],
   "source": [
    "class SimpleModel:\n",
    "    def __init__(self, model_name, framework):\n",
    "        # Attributes: Data stored in the object\n",
    "        self.model_name = model_name\n",
    "        self.framework = framework\n",
    "\n",
    "    def get_info(self):\n",
    "        # Method: Logic performed by the object\n",
    "        return f\"Model: {self.model_name} built with {self.framework}\"\n",
    "\n",
    "# Implementation\n",
    "my_model = SimpleModel(\"Iris-Classifier\", \"Scikit-Learn\")\n",
    "print(my_model.get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1c806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session using gpt-4o with 2 messages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is MLOps?'},\n",
       " {'role': 'assistant', 'content': 'MLOps is DevOps for Machine Learning.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, model_id: str):\n",
    "        self.model_id: str = model_id\n",
    "        # State: Keeping track of the conversation\n",
    "        self.history: List[Dict[str, str]] = [] \n",
    "\n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_session_summary(self) -> str:\n",
    "        return f\"Session using {self.model_id} with {len(self.history)} messages.\"\n",
    "\n",
    "# Implementation\n",
    "chat = ChatSession(\"gpt-4o\")\n",
    "chat.add_message(\"user\", \"What is MLOps?\")\n",
    "chat.add_message(\"assistant\", \"MLOps is DevOps for Machine Learning.\")\n",
    "print(chat.get_session_summary())\n",
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to https://mlflow.company.com...\n",
      "[METRIC] Deployment: 285443b9-6340-49b7-9bab-62cdce2a2fcb | Latency: 0.0000s\n",
      "Inference result for version v2.1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "class ProductionLLM:\n",
    "    def __init__(self, registry_uri: str, model_version: str):\n",
    "        self.registry_uri = registry_uri\n",
    "        self.version = model_version\n",
    "        self.deployment_id = str(uuid.uuid4())\n",
    "        self.__is_authenticated = False # Encapsulation: Private attribute\n",
    "\n",
    "    def __authenticate(self):\n",
    "        # Private method for internal logic only\n",
    "        print(f\"Connecting to {self.registry_uri}...\")\n",
    "        self.__is_authenticated = True\n",
    "\n",
    "    def predict(self, input_data: str):\n",
    "        if not self.__is_authenticated:\n",
    "            self.__authenticate()\n",
    "        \n",
    "        # Professional models include telemetry (timing/logging)\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Simulate model inference\n",
    "        result = f\"Inference result for version {self.version}\"\n",
    "        \n",
    "        latency = time.perf_counter() - start_time\n",
    "        self._log_metrics(latency) # Internal logging\n",
    "        return result\n",
    "\n",
    "    def _log_metrics(self, latency: float):\n",
    "        # In prod, this would send data to Grafana or Prometheus\n",
    "        print(f\"[METRIC] Deployment: {self.deployment_id} | Latency: {latency:.4f}s\")\n",
    "\n",
    "# Implementation\n",
    "llm_service = ProductionLLM(\"https://mlflow.company.com\", \"v2.1.0\")\n",
    "output = llm_service.predict(\"Analyze this log file.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf214f9",
   "metadata": {},
   "source": [
    "### Inheritence, Polymorphism, Encapsulation, Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65759cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Model\n",
      "[GPT-4o] generating: Hello\n"
     ]
    }
   ],
   "source": [
    "#----- Inheritence ----#\n",
    "class BaseModel:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.type = \"AI Model\"\n",
    "\n",
    "class LLM(BaseModel):\n",
    "    def generate(self, prompt):\n",
    "        return f\"[{self.name}] generating: {prompt}\"\n",
    "\n",
    "# Implementation\n",
    "my_llm = LLM(\"GPT-4o\")\n",
    "print(my_llm.type) # Inherited from BaseModel\n",
    "print(my_llm.generate(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16873fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI processing: Summarize text\n"
     ]
    }
   ],
   "source": [
    "class ModelProvider:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.__api_key = api_key  # ENCAPSULATION: Double underscore makes it private\n",
    "\n",
    "    def get_key_prefix(self):\n",
    "        return self.__api_key[:4] + \"****\"\n",
    "\n",
    "    def execute(self, task: str):\n",
    "        pass # To be overridden\n",
    "\n",
    "class OpenAI(ModelProvider):\n",
    "    def execute(self, task: str): # POLYMORPHISM: Different logic, same method name\n",
    "        return f\"OpenAI processing: {task}\"\n",
    "\n",
    "class Anthropic(ModelProvider):\n",
    "    def execute(self, task: str): # POLYMORPHISM\n",
    "        return f\"Claude processing: {task}\"\n",
    "\n",
    "# Usage\n",
    "def run_pipeline(provider: ModelProvider, task: str):\n",
    "    print(provider.execute(task)) # The pipeline doesn't care which provider it is\n",
    "\n",
    "run_pipeline(OpenAI(\"sk-123\"), \"Summarize text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3364e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 70B parameters from S3...\n",
      "{'status': 200, 'output': 'Llama response'}\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# ABSTRACTION: Defining the contract\n",
    "class LLMService(ABC):\n",
    "    @abstractmethod\n",
    "    def load_weights(self):\n",
    "        \"\"\"Must be implemented by any production model class\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def infer(self, payload: dict) -> dict:\n",
    "        \"\"\"Standard input/output format for all models in the company\"\"\"\n",
    "        pass\n",
    "\n",
    "class LlamaProduction(LLMService):\n",
    "    def load_weights(self):\n",
    "        print(\"Loading 70B parameters from S3...\")\n",
    "\n",
    "    def infer(self, payload: dict) -> dict:\n",
    "        # Complex logic hidden from the user\n",
    "        return {\"status\": 200, \"output\": \"Llama response\"}\n",
    "\n",
    "# Implementation\n",
    "# model = LLMService() # This would throw an error! You cannot instantiate an abstract class.\n",
    "prod_model = LlamaProduction()\n",
    "prod_model.load_weights()\n",
    "print(prod_model.infer({\"prompt\": \"Explain LLMOps\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb999818",
   "metadata": {},
   "source": [
    "### Decorators, Class methods, static methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827622ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SimpleModel at 0x25365721940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleModel:\n",
    "    @staticmethod\n",
    "    def validate_input(data):\n",
    "        # A simple helper that doesn't need 'self'\n",
    "        return isinstance(data, str)\n",
    "\n",
    "    @classmethod\n",
    "    def create_basic(cls):\n",
    "        # A factory method to create a default version\n",
    "        return cls(name=\"Default-Model\")\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "# Implementation\n",
    "is_valid = SimpleModel.validate_input(\"Hello\") # Static call\n",
    "model = SimpleModel.create_basic()  # Class call\n",
    "print(is_valid)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338de986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: 0.5003s\n",
      "Response from gpt-4-turbo\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# A Decorator to measure LLM Latency\n",
    "def time_it(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Inference Latency: {end - start:.4f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class LLMHandler:\n",
    "    def __init__(self, version):\n",
    "        self.version = version\n",
    "\n",
    "    @classmethod\n",
    "    def from_version(cls, version_num: int):\n",
    "        # Factory: Creates a specific model based on version\n",
    "        return cls(f\"gpt-{version_num}-turbo\")\n",
    "\n",
    "    @time_it # Applying the decorator\n",
    "    def generate(self, prompt):\n",
    "        time.sleep(0.5) # Simulate API delay\n",
    "        return f\"Response from {self.version}\"\n",
    "\n",
    "# Implementation\n",
    "llm = LLMHandler.from_version(4)\n",
    "print(llm.generate(\"What is LLMOps?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ee899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before function\n",
      "Hello World\n",
      "After function\n"
     ]
    }
   ],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Before function\")\n",
    "        func()\n",
    "        print(\"After function\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def hello():\n",
    "    print(\"Hello World\")\n",
    "\n",
    "hello()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ea940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling add\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def log(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"Calling {func.__name__}\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "@log\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "print(add(3, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e9e0456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model took 2.00s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"{func.__name__} took {time.time()-start:.2f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@track_time\n",
    "def train_model():\n",
    "    time.sleep(2)\n",
    "\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e18fe8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IIT\n"
     ]
    }
   ],
   "source": [
    "class Student:\n",
    "    college = \"ABC\"\n",
    "\n",
    "    @classmethod\n",
    "    def change_college(cls, name):\n",
    "        cls.college = name\n",
    "\n",
    "Student.change_college(\"IIT\")\n",
    "print(Student.college)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd865b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Model at 0x25365722120>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(config[\"name\"])\n",
    "\n",
    "model = Model.from_config({\"name\": \"XGBoost\"})\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12417cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    registry = {}\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, name, model):\n",
    "        cls.registry[name] = model\n",
    "\n",
    "ModelRegistry.register(\"v1\", \"LinearModel\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b6e2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "class Math:\n",
    "    @staticmethod\n",
    "    def add(a, b):\n",
    "        return a + b\n",
    "\n",
    "print(Math.add(2, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b372c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils:\n",
    "    @staticmethod\n",
    "    def normalize(x):\n",
    "        return (x - min(x)) / (max(x) - min(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e7accd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started: generate\n",
      "INFO:root:Finished: generate in 0.0017s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-llama-7b] response to: 'explain decorators in python'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import functools\n",
    "import logging\n",
    "\n",
    "# ---------- LOGGING CONFIG ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "# ---------- DECORATOR ----------\n",
    "def monitor(func):\n",
    "    \"\"\"\n",
    "    Decorator to monitor execution time and errors\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logging.info(f\"Started: {func.__name__}\")\n",
    "            result = func(*args, **kwargs)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in {func.__name__}: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            elapsed = time.time() - start_time\n",
    "            logging.info(f\"Finished: {func.__name__} in {elapsed:.4f}s\")\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# ---------- LLM SERVICE ----------\n",
    "class LLMService:\n",
    "    service_version = \"1.0.0\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: dict):\n",
    "        \"\"\"\n",
    "        Factory method\n",
    "        \"\"\"\n",
    "        if \"model_name\" not in config:\n",
    "            raise ValueError(\"config must contain 'model_name'\")\n",
    "        return cls(model_name=config[\"model_name\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(prompt: str) -> str:\n",
    "        return prompt.strip().lower()\n",
    "\n",
    "    @monitor\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        processed_prompt = self.preprocess(prompt)\n",
    "        response = f\"[{self.model_name}] response to: '{processed_prompt}'\"\n",
    "        return response\n",
    "\n",
    "\n",
    "# ---------- STEP 6: USAGE ----------\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"model_name\": \"gpt-llama-7b\"\n",
    "    }\n",
    "\n",
    "    llm = LLMService.from_config(config)\n",
    "\n",
    "    output = llm.generate(\"  Explain Decorators in Python  \")\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b8a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Hello   World\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# PROFESSIONAL: Retry Decorator\n",
    "def retry_on_failure(retries=3):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for i in range(retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {i+1} failed. Retrying...\")\n",
    "                    if i == retries - 1: raise e\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class ProductionInference:\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> str:\n",
    "        # Professional: Cleaning input before it hits the expensive LLM\n",
    "        return text.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "    @retry_on_failure(retries=2)\n",
    "    def call_api(self, cleaned_text):\n",
    "        if random.random() < 0.5: # Simulate 50% failure rate\n",
    "            raise ConnectionError(\"API Timeout\")\n",
    "        return f\"Success: {cleaned_text}\"\n",
    "\n",
    "# Implementation\n",
    "service = ProductionInference()\n",
    "input_data = \"   Hello \\n World   \"\n",
    "ready_data = ProductionInference.preprocess(input_data)\n",
    "print(service.call_api(ready_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d91d65",
   "metadata": {},
   "source": [
    "#### Magic/Dunder methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a24f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Sentiment-Analyzer\n",
      "SimpleModel(name='Sentiment-Analyzer')\n"
     ]
    }
   ],
   "source": [
    "class SimpleModel:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self):\n",
    "        # Professional: Clean string for users\n",
    "        return f\"Model: {self.name}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Professional: Detailed string for developers/logs\n",
    "        return f\"SimpleModel(name='{self.name}')\"\n",
    "\n",
    "# Implementation\n",
    "model = SimpleModel(\"Sentiment-Analyzer\")\n",
    "print(str(model))  # Output: Model: Sentiment-Analyzer\n",
    "print(repr(model)) # Output: SimpleModel(name='Sentiment-Analyzer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8206534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from s3://models/llama-3-v1...\n",
      "Processed: Hi using s3://models/llama-3-v1\n"
     ]
    }
   ],
   "source": [
    "class InferenceEngine:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        print(f\"Loading weights from {self.model_path}...\")\n",
    "\n",
    "    def __call__(self, input_text: str):\n",
    "        # This makes the object callable: engine(\"hello\")\n",
    "        return f\"Processed: {input_text} using {self.model_path}\"\n",
    "\n",
    "# Implementation\n",
    "engine = InferenceEngine(\"s3://models/llama-3-v1\")\n",
    "# Instead of engine.predict(\"Hi\"), we do:\n",
    "response = engine(\"Hi\") \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81116570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening connection to Data Lake...\n",
      "Dataset Size: 2\n",
      "First Item: {'input': 'What is AI?', 'label': 'processed'}\n",
      "Closing connection to Data Lake...\n"
     ]
    }
   ],
   "source": [
    "class LLMDataset:\n",
    "    def __init__(self, data_list: list):\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        # Allows use of len(dataset)\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Allows use of dataset[0] or for loops\n",
    "        sample = self.data[index]\n",
    "        return {\"input\": sample, \"label\": \"processed\"}\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Context Manager: Logic when entering 'with' block\n",
    "        print(\"Opening connection to Data Lake...\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Context Manager: Logic when exiting 'with' block\n",
    "        print(\"Closing connection to Data Lake...\")\n",
    "\n",
    "# Implementation\n",
    "raw_data = [\"What is AI?\", \"How does MLOps work?\"]\n",
    "with LLMDataset(raw_data) as ds:\n",
    "    print(f\"Dataset Size: {len(ds)}\")\n",
    "    print(f\"First Item: {ds[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c83470",
   "metadata": {},
   "source": [
    "### Reading and writing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13cbf34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Quantum Physics\n",
      "Write a poem about MLOps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Writing a list of prompts to a file\n",
    "prompts = [\"Explain Quantum Physics\", \"Write a poem about MLOps\"]\n",
    "\n",
    "with open(\"prompts.txt\", \"w\") as f:\n",
    "    for p in prompts:\n",
    "        f.write(p + \"\\n\")\n",
    "\n",
    "# Reading them back\n",
    "with open(\"prompts.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3fde456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User said: Hi\n",
      "User said: Bye\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Writing structured training data (JSONL format)\n",
    "training_data = [\n",
    "    {\"prompt\": \"Hi\", \"completion\": \"Hello!\"},\n",
    "    {\"prompt\": \"Bye\", \"completion\": \"Goodbye!\"}\n",
    "]\n",
    "\n",
    "with open(\"dataset.jsonl\", \"w\") as f:\n",
    "    for entry in training_data:\n",
    "        # Serialize dict to string and write as a new line\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Reading specific fields from JSONL\n",
    "with open(\"dataset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        print(f\"User said: {data['prompt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e343fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading artifact (107 bytes)...\n",
      "Successfully loaded model version: v1.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class ModelArtifact:\n",
    "    def __init__(self, weights, version):\n",
    "        self.weights = weights\n",
    "        self.version = version\n",
    "\n",
    "# Implementation: Saving a model object as a binary file\n",
    "my_model = ModelArtifact(weights=[0.1, 0.5, -0.2], version=\"v1.0\")\n",
    "\n",
    "# PROFESSIONAL: Binary Write ('wb')\n",
    "with open(\"model_v1.bin\", \"wb\") as f:\n",
    "    pickle.dump(my_model, f)\n",
    "\n",
    "# PROFESSIONAL: Checking file size before loading (Observability)\n",
    "file_size = os.path.getsize(\"model_v1.bin\")\n",
    "print(f\"Loading artifact ({file_size} bytes)...\")\n",
    "\n",
    "# Binary Read ('rb')\n",
    "with open(\"model_v1.bin\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "    print(f\"Successfully loaded model version: {loaded_model.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0507fe",
   "metadata": {},
   "source": [
    "### File Handling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c91119c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Position: 14\n",
      "Re-reading content: MLOps Revision\n"
     ]
    }
   ],
   "source": [
    "with open(\"notes.txt\", \"w+\") as f:\n",
    "    f.write(\"MLOps Revision\")\n",
    "    \n",
    "    # .tell() shows we are at the end of the file\n",
    "    print(f\"Current Position: {f.tell()}\") \n",
    "    \n",
    "    # .seek(0) moves cursor back to the start\n",
    "    f.seek(0)\n",
    "    print(f\"Re-reading content: {f.read()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84262b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No weights found. Starting training...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# INTERMEDIATE: Creating a robust directory structure for model versions\n",
    "base_path = Path(\"models\") / \"llama3\" / \"v1\"\n",
    "\n",
    "# Create directory if it doesn't exist (equivalent to mkdir -p)\n",
    "base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_file = base_path / \"weights.bin\"\n",
    "\n",
    "if model_file.exists():\n",
    "    print(f\"Loading weights from: {model_file}\")\n",
    "else:\n",
    "    print(\"No weights found. Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "583d20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resume_processing(file_path, last_position=0):\n",
    "#     with open(file_path, \"r\") as f:\n",
    "#         # PROFESSIONAL: Jump directly to the last processed byte\n",
    "#         f.seek(last_position)\n",
    "        \n",
    "#         # Process the next chunk\n",
    "#         new_data = f.readline()\n",
    "        \n",
    "#         # Save the new position for the next checkpoint\n",
    "#         current_pos = f.tell()\n",
    "#         return new_data, current_pos\n",
    "\n",
    "# # Implementation\n",
    "# # If we previously stopped at byte 5432...\n",
    "# data, next_pos = resume_processing(\"huge_dataset.jsonl\", last_position=5432)\n",
    "# print(f\"Resumed at {next_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b408a4",
   "metadata": {},
   "source": [
    "### Exception Handling: tryexcept and custom exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "930a078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: tell me a joke\n",
      "Skipped an item due to error: 'int' object has no attribute 'lower'\n",
      "Processing: what is ai?\n"
     ]
    }
   ],
   "source": [
    "# A simple list of inputs where one is invalid\n",
    "prompts = [\"Tell me a joke\", 12345, \"What is AI?\"]\n",
    "\n",
    "for p in prompts:\n",
    "    try:\n",
    "        # This will fail if p is not a string\n",
    "        print(f\"Processing: {p.lower()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped an item due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1f82145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: Model must be a .bin file\n",
      "Cleanup: Ensuring system resources are stable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_local_model(path: str):\n",
    "    try:\n",
    "        if not path.endswith(\".bin\"):\n",
    "            raise ValueError(\"Model must be a .bin file\")\n",
    "        \n",
    "        # Simulating opening a large file\n",
    "        print(f\"Opening {path}...\")\n",
    "        # f = open(path, 'rb')\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The model file does not exist at that path.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Validation Error: {ve}\")\n",
    "    finally:\n",
    "        print(\"Cleanup: Ensuring system resources are stable.\")\n",
    "\n",
    "load_local_model(\"llama.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51f77d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIMARY FAILED: OpenAI API: Rate limit reached (429).\n",
      "ACTION: Switching to local Llama-3 fallback...\n"
     ]
    }
   ],
   "source": [
    "# PROFESSIONAL: Custom Exception Classes\n",
    "class LLMError(Exception):\n",
    "    \"\"\"Base class for all LLM-related errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class RateLimitError(LLMError):\n",
    "    \"\"\"Raised when the API provider throttles requests.\"\"\"\n",
    "    pass\n",
    "\n",
    "class SafetyFilterError(LLMError):\n",
    "    \"\"\"Raised when the model refuses to answer due to safety.\"\"\"\n",
    "    pass\n",
    "\n",
    "def call_primary_llm(prompt):\n",
    "    # Simulate a 429 Rate Limit error from an API\n",
    "    raise RateLimitError(\"OpenAI API: Rate limit reached (429).\")\n",
    "\n",
    "# Implementation: Resilient Pipeline\n",
    "user_prompt = \"Generate a strategy for MLOps.\"\n",
    "\n",
    "try:\n",
    "    response = call_primary_llm(user_prompt)\n",
    "except RateLimitError as e:\n",
    "    print(f\"PRIMARY FAILED: {e}\")\n",
    "    print(\"ACTION: Switching to local Llama-3 fallback...\")\n",
    "    # response = call_local_llama(user_prompt)\n",
    "except SafetyFilterError:\n",
    "    print(\"ACTION: Notifying user that the request was blocked.\")\n",
    "except LLMError:\n",
    "    print(\"ACTION: General LLM failure. Check system logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426bd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rohit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
