{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b84f52b",
   "metadata": {},
   "source": [
    "### Revision of Python Intermediate level and Above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfdfc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Iris-Classifier built with Scikit-Learn\n"
     ]
    }
   ],
   "source": [
    "class SimpleModel:\n",
    "    def __init__(self, model_name, framework):\n",
    "        # Attributes: Data stored in the object\n",
    "        self.model_name = model_name\n",
    "        self.framework = framework\n",
    "\n",
    "    def get_info(self):\n",
    "        # Method: Logic performed by the object\n",
    "        return f\"Model: {self.model_name} built with {self.framework}\"\n",
    "\n",
    "# Implementation\n",
    "my_model = SimpleModel(\"Iris-Classifier\", \"Scikit-Learn\")\n",
    "print(my_model.get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1c806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session using gpt-4o with 2 messages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is MLOps?'},\n",
       " {'role': 'assistant', 'content': 'MLOps is DevOps for Machine Learning.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, model_id: str):\n",
    "        self.model_id: str = model_id\n",
    "        # State: Keeping track of the conversation\n",
    "        self.history: List[Dict[str, str]] = [] \n",
    "\n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_session_summary(self) -> str:\n",
    "        return f\"Session using {self.model_id} with {len(self.history)} messages.\"\n",
    "\n",
    "# Implementation\n",
    "chat = ChatSession(\"gpt-4o\")\n",
    "chat.add_message(\"user\", \"What is MLOps?\")\n",
    "chat.add_message(\"assistant\", \"MLOps is DevOps for Machine Learning.\")\n",
    "print(chat.get_session_summary())\n",
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to https://mlflow.company.com...\n",
      "[METRIC] Deployment: 285443b9-6340-49b7-9bab-62cdce2a2fcb | Latency: 0.0000s\n",
      "Inference result for version v2.1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "class ProductionLLM:\n",
    "    def __init__(self, registry_uri: str, model_version: str):\n",
    "        self.registry_uri = registry_uri\n",
    "        self.version = model_version\n",
    "        self.deployment_id = str(uuid.uuid4())\n",
    "        self.__is_authenticated = False # Encapsulation: Private attribute\n",
    "\n",
    "    def __authenticate(self):\n",
    "        # Private method for internal logic only\n",
    "        print(f\"Connecting to {self.registry_uri}...\")\n",
    "        self.__is_authenticated = True\n",
    "\n",
    "    def predict(self, input_data: str):\n",
    "        if not self.__is_authenticated:\n",
    "            self.__authenticate()\n",
    "        \n",
    "        # Professional models include telemetry (timing/logging)\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Simulate model inference\n",
    "        result = f\"Inference result for version {self.version}\"\n",
    "        \n",
    "        latency = time.perf_counter() - start_time\n",
    "        self._log_metrics(latency) # Internal logging\n",
    "        return result\n",
    "\n",
    "    def _log_metrics(self, latency: float):\n",
    "        # In prod, this would send data to Grafana or Prometheus\n",
    "        print(f\"[METRIC] Deployment: {self.deployment_id} | Latency: {latency:.4f}s\")\n",
    "\n",
    "# Implementation\n",
    "llm_service = ProductionLLM(\"https://mlflow.company.com\", \"v2.1.0\")\n",
    "output = llm_service.predict(\"Analyze this log file.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf214f9",
   "metadata": {},
   "source": [
    "### Inheritence, Polymorphism, Encapsulation, Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65759cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Model\n",
      "[GPT-4o] generating: Hello\n"
     ]
    }
   ],
   "source": [
    "#----- Inheritence ----#\n",
    "class BaseModel:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.type = \"AI Model\"\n",
    "\n",
    "class LLM(BaseModel):\n",
    "    def generate(self, prompt):\n",
    "        return f\"[{self.name}] generating: {prompt}\"\n",
    "\n",
    "# Implementation\n",
    "my_llm = LLM(\"GPT-4o\")\n",
    "print(my_llm.type) # Inherited from BaseModel\n",
    "print(my_llm.generate(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16873fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI processing: Summarize text\n"
     ]
    }
   ],
   "source": [
    "class ModelProvider:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.__api_key = api_key  # ENCAPSULATION: Double underscore makes it private\n",
    "\n",
    "    def get_key_prefix(self):\n",
    "        return self.__api_key[:4] + \"****\"\n",
    "\n",
    "    def execute(self, task: str):\n",
    "        pass # To be overridden\n",
    "\n",
    "class OpenAI(ModelProvider):\n",
    "    def execute(self, task: str): # POLYMORPHISM: Different logic, same method name\n",
    "        return f\"OpenAI processing: {task}\"\n",
    "\n",
    "class Anthropic(ModelProvider):\n",
    "    def execute(self, task: str): # POLYMORPHISM\n",
    "        return f\"Claude processing: {task}\"\n",
    "\n",
    "# Usage\n",
    "def run_pipeline(provider: ModelProvider, task: str):\n",
    "    print(provider.execute(task)) # The pipeline doesn't care which provider it is\n",
    "\n",
    "run_pipeline(OpenAI(\"sk-123\"), \"Summarize text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3364e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 70B parameters from S3...\n",
      "{'status': 200, 'output': 'Llama response'}\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# ABSTRACTION: Defining the contract\n",
    "class LLMService(ABC):\n",
    "    @abstractmethod\n",
    "    def load_weights(self):\n",
    "        \"\"\"Must be implemented by any production model class\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def infer(self, payload: dict) -> dict:\n",
    "        \"\"\"Standard input/output format for all models in the company\"\"\"\n",
    "        pass\n",
    "\n",
    "class LlamaProduction(LLMService):\n",
    "    def load_weights(self):\n",
    "        print(\"Loading 70B parameters from S3...\")\n",
    "\n",
    "    def infer(self, payload: dict) -> dict:\n",
    "        # Complex logic hidden from the user\n",
    "        return {\"status\": 200, \"output\": \"Llama response\"}\n",
    "\n",
    "# Implementation\n",
    "# model = LLMService() # This would throw an error! You cannot instantiate an abstract class.\n",
    "prod_model = LlamaProduction()\n",
    "prod_model.load_weights()\n",
    "print(prod_model.infer({\"prompt\": \"Explain LLMOps\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb999818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
